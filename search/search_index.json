{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"From Docker to Kubernetes - Multi-Arch with Go and Nodejs \u00b6 This guide has 5 main parts: Getting Official with Multi-arch Images Learning How to Build Best-practice Node.js Docker Images Guide to Building Best-practice Go Docker Images Building Multi-arch Docker Images Kubernetes Time Proxy Users here is your extra part to start with: Hello Proxy Users Who is a proxy user? A proxy user is an individual connecting to the internet via a proxy server . This setup is done for network security in enterprise environments and requires setting proxy variables such as http_proxy , etc. If you are behind a proxy server, you would most likely know about it due to the need to change settings to get various applications to work. Therefore, if you don't think you are using a proxy, you most likely are not. However, if you are really unsure about this, this article can help you find out for certain. Prerequisites \u00b6 Have an up and running Kubernetes cluster Setup access to that cluster using kubectl Register for a DockerHub account following the process outlined on DockerHub here Download Docker on your workstation from DockerHub using your newly created free account here What will this Guide do for you? \u00b6 This guide will start by looking at what Official Repositories are and how to build from them. Then, it will shine light on how to tell if an app will run on your platform (architecture). Next, we will look at how best to build images for go and nodejs in Docker with examples and then actually build these images with Multi-Architecture manifests. Finally, we will use these images or my pre-built images to deploy to a Kubernetes cluster. What in the Tarnation is a Container? \u00b6 Application containers make use of linux kernel features to provide lightweight isolation by limiting what a process (running program) can see ( namespaces ), what system resources a process can use ( cgroups ), and what system calls a process can make ( seccomp ) combined with other features such as enhanced access control ( SELinux ). We use container images (all the dependencies and files our application needs to run put together in layers), which we can bundle and store in image repositories to use to start containers (run a process using the files from the aforementioned container image (image) tar file in an environment with the isolation described above). If you just finished reading the above and want to get a little deeper into some of the parts of containers I just name-dropped please have a look see at What even is a container: namespaces and cgroups . If you want a technical deep-dive into containers and container runtimes getting into the nitty-gritty, the fabulous 4 part series Container Runtimes is the happy path for you. Get the Code \u00b6 The code for this guide is on the accompanying GitHub . You can also access this page by clicking on the GitHub icon on the top right of every page. If you have git installed, it can be brought onto your computer with: git clone https://github.com/siler23/MultiArchDockerKubernetes.git If you don't have git or don't want to clone this repo, you can use the download link in the top right corner of the GitHub page. Search to your Heart's Content \u00b6 Search the docs for the content you want by using the search bar at the top of each page like so for hello world. Tip You can also immediately jump to the search bar by typing f or s . Let's Begin \u00b6 Non-Proxy Users (Regular Users) Start Here: Official Docker Repos and Multi-Arch Primer Proxy Users Start Here: Proxy PSA for Proxy Users Places to go and things to see in the Future \u00b6 Possible Next Steps \u00b6 Packaging Applications and Services with Kubernetes Operators Red Hat OpenShift Free Interactive Online Learning IBM Guidance on Containers on IBM Z \u00b6 IBM Guidance on running Docker containers on IBM Z PDF Shout-out to Mkdocs \u00b6 I made this site with mkdocs using the material theme.","title":"Home"},{"location":"#from-docker-to-kubernetes-multi-arch-with-go-and-nodejs","text":"This guide has 5 main parts: Getting Official with Multi-arch Images Learning How to Build Best-practice Node.js Docker Images Guide to Building Best-practice Go Docker Images Building Multi-arch Docker Images Kubernetes Time Proxy Users here is your extra part to start with: Hello Proxy Users Who is a proxy user? A proxy user is an individual connecting to the internet via a proxy server . This setup is done for network security in enterprise environments and requires setting proxy variables such as http_proxy , etc. If you are behind a proxy server, you would most likely know about it due to the need to change settings to get various applications to work. Therefore, if you don't think you are using a proxy, you most likely are not. However, if you are really unsure about this, this article can help you find out for certain.","title":"From Docker to Kubernetes - Multi-Arch with Go and Nodejs"},{"location":"#prerequisites","text":"Have an up and running Kubernetes cluster Setup access to that cluster using kubectl Register for a DockerHub account following the process outlined on DockerHub here Download Docker on your workstation from DockerHub using your newly created free account here","title":"Prerequisites"},{"location":"#what-will-this-guide-do-for-you","text":"This guide will start by looking at what Official Repositories are and how to build from them. Then, it will shine light on how to tell if an app will run on your platform (architecture). Next, we will look at how best to build images for go and nodejs in Docker with examples and then actually build these images with Multi-Architecture manifests. Finally, we will use these images or my pre-built images to deploy to a Kubernetes cluster.","title":"What will this Guide do for you?"},{"location":"#what-in-the-tarnation-is-a-container","text":"Application containers make use of linux kernel features to provide lightweight isolation by limiting what a process (running program) can see ( namespaces ), what system resources a process can use ( cgroups ), and what system calls a process can make ( seccomp ) combined with other features such as enhanced access control ( SELinux ). We use container images (all the dependencies and files our application needs to run put together in layers), which we can bundle and store in image repositories to use to start containers (run a process using the files from the aforementioned container image (image) tar file in an environment with the isolation described above). If you just finished reading the above and want to get a little deeper into some of the parts of containers I just name-dropped please have a look see at What even is a container: namespaces and cgroups . If you want a technical deep-dive into containers and container runtimes getting into the nitty-gritty, the fabulous 4 part series Container Runtimes is the happy path for you.","title":"What in the Tarnation is a Container?"},{"location":"#get-the-code","text":"The code for this guide is on the accompanying GitHub . You can also access this page by clicking on the GitHub icon on the top right of every page. If you have git installed, it can be brought onto your computer with: git clone https://github.com/siler23/MultiArchDockerKubernetes.git If you don't have git or don't want to clone this repo, you can use the download link in the top right corner of the GitHub page.","title":"Get the Code"},{"location":"#search-to-your-hearts-content","text":"Search the docs for the content you want by using the search bar at the top of each page like so for hello world. Tip You can also immediately jump to the search bar by typing f or s .","title":"Search to your Heart's Content"},{"location":"#lets-begin","text":"Non-Proxy Users (Regular Users) Start Here: Official Docker Repos and Multi-Arch Primer Proxy Users Start Here: Proxy PSA for Proxy Users","title":"Let's Begin"},{"location":"#places-to-go-and-things-to-see-in-the-future","text":"","title":"Places to go and things to see in the Future"},{"location":"#possible-next-steps","text":"Packaging Applications and Services with Kubernetes Operators Red Hat OpenShift Free Interactive Online Learning","title":"Possible Next Steps"},{"location":"#ibm-guidance-on-containers-on-ibm-z","text":"IBM Guidance on running Docker containers on IBM Z PDF","title":"IBM Guidance on Containers on IBM Z"},{"location":"#shout-out-to-mkdocs","text":"I made this site with mkdocs using the material theme.","title":"Shout-out to Mkdocs"},{"location":"0-ProxyPSA/","text":"Hello Proxy Users \u00b6 We will show how to use a proxy for Docker, how to use a proxy throughout this guide and give links to great proxy user articles. Docker proxy for push/pull \u00b6 To setup Docker proxies for docker pull and docker push , you will follow different steps for the different versions of Docker. Windows/mac Use the UI. Navigate to Docker Settings in Windows (Preferences in Mac) and fill in the proxy section with your manual proxy configuration like so: For more information on setting proxies in Docker for Windows see Setting Docker Proxies for Windows Linux Make a docker.service.d file and restart Docker as below: mkdir -p /etc/systemd/system/docker.service.d cat > /etc/systemd/system/docker.service.d/http-proxy.conf << EOF [Service] Environment=\"HTTP_PROXY=http://myproxy:8080\" Environment=\"HTTPS_PROXY=http://myproxy:8080\" Environment=\"NO_PROXY=127.0.0.1,localhost EOF systemctl daemon-reload systemctl restart docker This article goes into detail on how to do this Setup your terminal/command prompt for Proxy \u00b6 If using a proxy, declare your environment variables to reference in commands. Here is an example with a proxy of http://myproxy:8080 and no proxy of localhost and 127.0.0.1. PSA Proxy must have either http:// or https:// at the beginning to be used. Otherwise it will be ignored. MAC/Linux http_proxy = http://myproxy:8080 https_proxy = http://myproxy:8080 no_proxy = \"localhost, 127.0.0.1\" Windows Command Prompt set http_proxy = http://myproxy:8080 set https_proxy = http://myproxy:8080 set no_proxy = localhost, 127 .0.0.1 Windows PowerShell $http_proxy = \"http://myproxy:8080\" $https_proxy = \"http://myproxy:8080\" $no_proxy = \"localhost, 127.0.0.1\" Building Docker images behind Proxy \u00b6 There are two main options for building Docker images behind a proxy so that your build can do things like use apt-get to install packages for the image, wget or curl to download files or downloading a git repository during the install itself. Use environment variables in your Dockerfile. Example ENV http_proxy http://docker.for.win.localhost:8888 ENV https_proxy http://docker.for.win.localhost:8888 ENV no_proxy localhost, 127 .0.0.1 This means that your settings persist to the Docker image itself. This is in many cases not what you want because this means your users by default will have your proxy settings. If you are using an image internally only then this automatically lets you run the image. However, if you are giving an image to someone who will use it outside of your proxy environment please use the next option for setting proxy so they don't have to worry about the app not working and then having to change the settings themselves when running the Docker container. Use build args Build args allow you to provide variables for use during building an image. When the container itself gets used these build args are not present in the container environment. This allows us to build an image using our proxy to do our apt-get installs and curls and then remove this variable so that the user can use their own web environment (proxy or no proxy) instead of by default trying to contact our proxy which they won't be able to reach. Example docker build -t href --build-arg http_proxy = http://docker.for.win.localhost:8888 --build-arg https_proxy = http://docker.for.win.localhost:8888 --build-arg no_proxy = localhost . Using our environment variables defined previously this becomes BASH (Mac/Linux) docker build -t href --build-arg http_proxy = $http_proxy --build-arg https_proxy = $https_proxy --build-arg no_proxy = \" $no_proxy \" . Windows (Command Prompt) docker build -t href --build-arg http_proxy= %http_proxy% --build-arg https_proxy= %https_proxy% --build-arg no_proxy= \" %no_proxy% \" . Now, our proxy environment variables don't show up to the end user. What about if I have confidential information such as a username/password in my proxy? If you have this info in your proxy environment variables users will be able to see it in the running container's environment as well as by inspecting the container. If you used build args the user will be able to see it by inspecting the container with docker inspect as well. So what do we do. Well, we can use multi-stage builds which I go over in 3-Best-Practice-go of this guide to accomplish the task since our final container will only have copies of the files built using the proxy in a previous container. To see how to actually do this look at Accessing Private Repository . You can follow the steps outlined in the article for your proxy with username/password declaration for security. Running Docker containers behind a proxy \u00b6 Some Docker container don't reach out to the outside world, so they don't need to worry about using a proxy. However, a lot of Docker containers need to reach out to do things such as checking information or sending notifications to websites. In order to do so behind a proxy the Docker container needs the correct environment variables set. The way to do that is when running the container use -e with the proxy variables: BASH (Mac/Linux) docker run --rm -e http_proxy = $http_proxy -e https_proxy = $https_proxy -e no_proxy = \" $no_proxy \" mplatform/mquery gmoney23/outyet Windows (Command Prompt) docker run --rm -e http_proxy= %http_proxy% -e https_proxy= %https_proxy% -e no_proxy= \" %no_proxy% \" mplatform/mquery gmoney23/outyet This passes the environment variables to the Docker container. The fact that somebody thought of you and your proxy situation, fills you with determination . To use a proxy with Kubernetes see 5-Deploy-to-Kubernetes \u00b6 Reference \u00b6 If you still have Docker proxy needs see this article Docker Behind a Corporate Firewall Onwards \u00b6 Part 1: Getting Official with Multi-arch Images \u00b6","title":"0. Hello Proxy Users"},{"location":"0-ProxyPSA/#hello-proxy-users","text":"We will show how to use a proxy for Docker, how to use a proxy throughout this guide and give links to great proxy user articles.","title":"Hello Proxy Users"},{"location":"0-ProxyPSA/#docker-proxy-for-pushpull","text":"To setup Docker proxies for docker pull and docker push , you will follow different steps for the different versions of Docker. Windows/mac Use the UI. Navigate to Docker Settings in Windows (Preferences in Mac) and fill in the proxy section with your manual proxy configuration like so: For more information on setting proxies in Docker for Windows see Setting Docker Proxies for Windows Linux Make a docker.service.d file and restart Docker as below: mkdir -p /etc/systemd/system/docker.service.d cat > /etc/systemd/system/docker.service.d/http-proxy.conf << EOF [Service] Environment=\"HTTP_PROXY=http://myproxy:8080\" Environment=\"HTTPS_PROXY=http://myproxy:8080\" Environment=\"NO_PROXY=127.0.0.1,localhost EOF systemctl daemon-reload systemctl restart docker This article goes into detail on how to do this","title":"Docker proxy for push/pull"},{"location":"0-ProxyPSA/#setup-your-terminalcommand-prompt-for-proxy","text":"If using a proxy, declare your environment variables to reference in commands. Here is an example with a proxy of http://myproxy:8080 and no proxy of localhost and 127.0.0.1. PSA Proxy must have either http:// or https:// at the beginning to be used. Otherwise it will be ignored. MAC/Linux http_proxy = http://myproxy:8080 https_proxy = http://myproxy:8080 no_proxy = \"localhost, 127.0.0.1\" Windows Command Prompt set http_proxy = http://myproxy:8080 set https_proxy = http://myproxy:8080 set no_proxy = localhost, 127 .0.0.1 Windows PowerShell $http_proxy = \"http://myproxy:8080\" $https_proxy = \"http://myproxy:8080\" $no_proxy = \"localhost, 127.0.0.1\"","title":"Setup your terminal/command prompt for Proxy"},{"location":"0-ProxyPSA/#building-docker-images-behind-proxy","text":"There are two main options for building Docker images behind a proxy so that your build can do things like use apt-get to install packages for the image, wget or curl to download files or downloading a git repository during the install itself. Use environment variables in your Dockerfile. Example ENV http_proxy http://docker.for.win.localhost:8888 ENV https_proxy http://docker.for.win.localhost:8888 ENV no_proxy localhost, 127 .0.0.1 This means that your settings persist to the Docker image itself. This is in many cases not what you want because this means your users by default will have your proxy settings. If you are using an image internally only then this automatically lets you run the image. However, if you are giving an image to someone who will use it outside of your proxy environment please use the next option for setting proxy so they don't have to worry about the app not working and then having to change the settings themselves when running the Docker container. Use build args Build args allow you to provide variables for use during building an image. When the container itself gets used these build args are not present in the container environment. This allows us to build an image using our proxy to do our apt-get installs and curls and then remove this variable so that the user can use their own web environment (proxy or no proxy) instead of by default trying to contact our proxy which they won't be able to reach. Example docker build -t href --build-arg http_proxy = http://docker.for.win.localhost:8888 --build-arg https_proxy = http://docker.for.win.localhost:8888 --build-arg no_proxy = localhost . Using our environment variables defined previously this becomes BASH (Mac/Linux) docker build -t href --build-arg http_proxy = $http_proxy --build-arg https_proxy = $https_proxy --build-arg no_proxy = \" $no_proxy \" . Windows (Command Prompt) docker build -t href --build-arg http_proxy= %http_proxy% --build-arg https_proxy= %https_proxy% --build-arg no_proxy= \" %no_proxy% \" . Now, our proxy environment variables don't show up to the end user. What about if I have confidential information such as a username/password in my proxy? If you have this info in your proxy environment variables users will be able to see it in the running container's environment as well as by inspecting the container. If you used build args the user will be able to see it by inspecting the container with docker inspect as well. So what do we do. Well, we can use multi-stage builds which I go over in 3-Best-Practice-go of this guide to accomplish the task since our final container will only have copies of the files built using the proxy in a previous container. To see how to actually do this look at Accessing Private Repository . You can follow the steps outlined in the article for your proxy with username/password declaration for security.","title":"Building Docker images behind Proxy"},{"location":"0-ProxyPSA/#running-docker-containers-behind-a-proxy","text":"Some Docker container don't reach out to the outside world, so they don't need to worry about using a proxy. However, a lot of Docker containers need to reach out to do things such as checking information or sending notifications to websites. In order to do so behind a proxy the Docker container needs the correct environment variables set. The way to do that is when running the container use -e with the proxy variables: BASH (Mac/Linux) docker run --rm -e http_proxy = $http_proxy -e https_proxy = $https_proxy -e no_proxy = \" $no_proxy \" mplatform/mquery gmoney23/outyet Windows (Command Prompt) docker run --rm -e http_proxy= %http_proxy% -e https_proxy= %https_proxy% -e no_proxy= \" %no_proxy% \" mplatform/mquery gmoney23/outyet This passes the environment variables to the Docker container. The fact that somebody thought of you and your proxy situation, fills you with determination .","title":"Running Docker containers behind a proxy"},{"location":"0-ProxyPSA/#to-use-a-proxy-with-kubernetes-see-5-deploy-to-kubernetes","text":"","title":"To use a proxy with Kubernetes see 5-Deploy-to-Kubernetes"},{"location":"0-ProxyPSA/#reference","text":"If you still have Docker proxy needs see this article Docker Behind a Corporate Firewall","title":"Reference"},{"location":"0-ProxyPSA/#onwards","text":"","title":"Onwards"},{"location":"0-ProxyPSA/#part-1-getting-official-with-multi-arch-images","text":"","title":"Part 1: Getting Official with Multi-arch Images"},{"location":"1-Official-Multiarch/","text":"1. Getting Official with Multi-arch Images \u00b6 This section goes through the official Docker repositories for building images and what multi-architecture images are and how to spot them. If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document. Official Repositories \u00b6 Docker Official Images are a special set of Docker repositories on DockerHub that host images for operating systems and base software that follow Dockerfile best practices and undergo regular security scans to allow them to serve as building blocks for your applications. These repositories are where you get the images to build your applications on top of. Check out the Docker Official Images here We will specifically be looking at building Node.js and Go applications, so we will use the Node.js Official Docker Image and the Golang Official Docker Image repositories. These images are multi-arch which means they support multiple architectures. All official images are multi-arch . How do we know if these images support both s390x and x86? If we look at the supported architectures under the Quick reference section on Docker Hub, we can see both images support both the x86 [amd64] and LinuxONE/z [s390x] architectures. :) Tip If not otherwise noted in documentation, containers are assumed to use Linux as the operating system. This means that if I run docker pull node on linux on Z, it will pull me the s390x image. Similarly, if I run docker pull node on x86, it will pull me the x86 image. Thus, once the image is built and set up as multi-arch, the difference is abstracted away from applications using the image (such as a node application built using the Node.js official image as the base) [i.e. FROM node works on both x86 and s390x]. How do Multi-Arch Images Work \u00b6 A Multi-Arch image consists of a manifest list . This manifest list links its image (i.e. each Node.js image on the Node.js Official Docker Repository ) to the image manifests of the docker images for the different architectures at s390x node image , amd64 node image , etc. This is how the magic happens, so instead of having to call the image for each architecture, I can just docker pull node and it gives me the correct architecture image. Nevertheless, there are official repositories that hold the official images for each architecture. This is where you can find the specific images linked to in the official image as in the example above. For IBM Z, we have: Official s390x images such as the s390x node and s390x golang images. For x86, we have official amd64 images such as the amd64 node and amd64 golang images Other architectures such as Power and arm32 have their own repositories as well. Checking Image Architecture \u00b6 Many times you will want to find out if an image supports your architecture. If it doesn't and you try to run it, you can get a nasty little error. Error standard_init_linux.go:185: exec user process caused \"exec format error\" This stops your container from working. In hopes to avoid that trouble once we've already spent the time to pull the image and try to run it, here are the best ways to check images for their architecture. I. If the image exists on your system use docker inspect \u00b6 Assume I ran previously: docker pull busybox Now, I can run: docker inspect -f '{{.Architecture}}' busybox But what if the image isn't on your system? docker inspect -f '{{.Architecture}}' alpine In order to fix this you would have to pull the image and then inspect it: docker pull alpine && docker inspect -f '{{.Architecture}}' alpine The problem with this is that you would potentially have to download some large images just to check if they are for your architecture, which is time-consuming and a waste of space. II. If the image isn't on your system, use the mplatform/mquery docker image \u00b6 REGULAR docker run --rm mplatform/mquery ibmcom/icp-nodejs-sample PROXY docker run --rm -e http_proxy= %http_proxy% -e https_proxy= %https_proxy% -e no_proxy= \" %no_proxy% \" mplatform/mquery ibmcom/icp-nodejs-sample where %http_proxy% , etc. are environment variables previously set in windows to the value of the http_proxy with set http_proxy=yourproxyaddress:yourproxyport. For mac/linux you would set with http_proxy=yourproxyaddress:yourproxyport and reference with $http_proxy The mquery image is \"A simple utility and backend for querying Docker v2 API-supporting registry images and reporting on manifest list multi-platform image support\" ( mquery project github page ) which tells us which architectures a given image supports by checking its manifest list. If it is an image with no manifest list, it will tell us that (and which arch it supports) instead. REGULAR docker run --rm mplatform/mquery s390x/node PROXY docker run --rm -e http_proxy= %http_proxy% -e https_proxy= %https_proxy% -e no_proxy= \" %no_proxy% \" mplatform/mquery s390x/node Note You can also use the manifest-tool itself to do this, but the manifest-tool needs to be installed first and gives more verbose output. The other alternative, the docker manifest inspect command doesn't work for all supported registries yet, (it continues to be improved) and needs to be enabled ( it's currently experimental ). Thus, using the mquery image is generally better for checking arch support quickly. Knowing that the exec format error can be a thing of the past fills you with determination . Next up, we will build some Node.js docker images and learn some Node.js docker best practices. Part 2: Learning How to Build Best-practice Node.js Docker Images \u00b6","title":"1. Getting Official with Multi-arch Images"},{"location":"1-Official-Multiarch/#1-getting-official-with-multi-arch-images","text":"This section goes through the official Docker repositories for building images and what multi-architecture images are and how to spot them. If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document.","title":"1. Getting Official with Multi-arch Images"},{"location":"1-Official-Multiarch/#official-repositories","text":"Docker Official Images are a special set of Docker repositories on DockerHub that host images for operating systems and base software that follow Dockerfile best practices and undergo regular security scans to allow them to serve as building blocks for your applications. These repositories are where you get the images to build your applications on top of. Check out the Docker Official Images here We will specifically be looking at building Node.js and Go applications, so we will use the Node.js Official Docker Image and the Golang Official Docker Image repositories. These images are multi-arch which means they support multiple architectures. All official images are multi-arch . How do we know if these images support both s390x and x86? If we look at the supported architectures under the Quick reference section on Docker Hub, we can see both images support both the x86 [amd64] and LinuxONE/z [s390x] architectures. :) Tip If not otherwise noted in documentation, containers are assumed to use Linux as the operating system. This means that if I run docker pull node on linux on Z, it will pull me the s390x image. Similarly, if I run docker pull node on x86, it will pull me the x86 image. Thus, once the image is built and set up as multi-arch, the difference is abstracted away from applications using the image (such as a node application built using the Node.js official image as the base) [i.e. FROM node works on both x86 and s390x].","title":"Official Repositories"},{"location":"1-Official-Multiarch/#how-do-multi-arch-images-work","text":"A Multi-Arch image consists of a manifest list . This manifest list links its image (i.e. each Node.js image on the Node.js Official Docker Repository ) to the image manifests of the docker images for the different architectures at s390x node image , amd64 node image , etc. This is how the magic happens, so instead of having to call the image for each architecture, I can just docker pull node and it gives me the correct architecture image. Nevertheless, there are official repositories that hold the official images for each architecture. This is where you can find the specific images linked to in the official image as in the example above. For IBM Z, we have: Official s390x images such as the s390x node and s390x golang images. For x86, we have official amd64 images such as the amd64 node and amd64 golang images Other architectures such as Power and arm32 have their own repositories as well.","title":"How do Multi-Arch Images Work"},{"location":"1-Official-Multiarch/#checking-image-architecture","text":"Many times you will want to find out if an image supports your architecture. If it doesn't and you try to run it, you can get a nasty little error. Error standard_init_linux.go:185: exec user process caused \"exec format error\" This stops your container from working. In hopes to avoid that trouble once we've already spent the time to pull the image and try to run it, here are the best ways to check images for their architecture.","title":"Checking Image Architecture"},{"location":"1-Official-Multiarch/#i-if-the-image-exists-on-your-system-use-docker-inspect","text":"Assume I ran previously: docker pull busybox Now, I can run: docker inspect -f '{{.Architecture}}' busybox But what if the image isn't on your system? docker inspect -f '{{.Architecture}}' alpine In order to fix this you would have to pull the image and then inspect it: docker pull alpine && docker inspect -f '{{.Architecture}}' alpine The problem with this is that you would potentially have to download some large images just to check if they are for your architecture, which is time-consuming and a waste of space.","title":"I. If the image exists on your system use docker inspect"},{"location":"1-Official-Multiarch/#ii-if-the-image-isnt-on-your-system-use-the-mplatformmquery-docker-image","text":"REGULAR docker run --rm mplatform/mquery ibmcom/icp-nodejs-sample PROXY docker run --rm -e http_proxy= %http_proxy% -e https_proxy= %https_proxy% -e no_proxy= \" %no_proxy% \" mplatform/mquery ibmcom/icp-nodejs-sample where %http_proxy% , etc. are environment variables previously set in windows to the value of the http_proxy with set http_proxy=yourproxyaddress:yourproxyport. For mac/linux you would set with http_proxy=yourproxyaddress:yourproxyport and reference with $http_proxy The mquery image is \"A simple utility and backend for querying Docker v2 API-supporting registry images and reporting on manifest list multi-platform image support\" ( mquery project github page ) which tells us which architectures a given image supports by checking its manifest list. If it is an image with no manifest list, it will tell us that (and which arch it supports) instead. REGULAR docker run --rm mplatform/mquery s390x/node PROXY docker run --rm -e http_proxy= %http_proxy% -e https_proxy= %https_proxy% -e no_proxy= \" %no_proxy% \" mplatform/mquery s390x/node Note You can also use the manifest-tool itself to do this, but the manifest-tool needs to be installed first and gives more verbose output. The other alternative, the docker manifest inspect command doesn't work for all supported registries yet, (it continues to be improved) and needs to be enabled ( it's currently experimental ). Thus, using the mquery image is generally better for checking arch support quickly. Knowing that the exec format error can be a thing of the past fills you with determination . Next up, we will build some Node.js docker images and learn some Node.js docker best practices.","title":"II. If the image isn't on your system, use the mplatform/mquery docker image"},{"location":"1-Official-Multiarch/#part-2-learning-how-to-build-best-practice-nodejs-docker-images","text":"","title":"Part 2: Learning How to Build Best-practice Node.js Docker Images"},{"location":"2-Best-Practice-Nodejs/","text":"2. Learning How to Build Best-practice Node.js Docker Images \u00b6 First, we will go over the icp-node-js-sample app and Dockerfile. Then, we will see Dockerfile best practices for nodejs using the basic hello world app from the Node.js site. Note We will build all of these images in Part 4. This is a guide explaining the ins and outs of making the files to do so [i.e. the foundation for success] Node.js Download for Later \u00b6 Here is the Node.js download if you want to run it locally to familiarize yourself with it/develop with it. For this guide, you actually don't need Node.js installed on your computer because of the magic of Docker. If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document. ICP Node.js Sample \u00b6 Here is the Dockerfile for the ICP Node.js Sample . Let's see what we got here folks: Run it with: docker run --rm -it -p 3000:3000 gmoney23/nodejs-sample Click on nodejs-sample (while it's running) to see it in your web browser. If on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. Here is what it will look like in the browser: Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. Why Should I Optimize my Nodejs Images? \u00b6 Security \u00b6 Running the container as root can potentially leave open security vulnerabilities which can be curtailed by running the container as a regular user. We will see how to do this in the example below. Additionally, smaller base images such as the alpine image below have a smaller attack surface in terms of vulnerable software (more software / packages = more potential vulnerabilities) and thus, regularly perform magnitudes better (have many times less vulnerabilities) in container security scans than base images of more traditional operating systems such as Ubuntu. Size \u00b6 By minimizing the size of images, we can prevent wasting resources for large base images and prevent using a ton of storage for all of our container images. Moreover, we can take better advantage of cache for our running containers. Node.js Hello World Server \u00b6 Here is the node-web-app Dockerfile where we can see comments for how to write a best practice Node.js Dockerfile. The simple code we're dockerizing for the web app comes from this Node.js guide Run it with: docker run --rm -it -p 8080:8080 gmoney23/node-web-app Click on node-web-app (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:8080 where serverip is your server's ip address. Here is what it will look like in the browser: Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. Here's what it will look like in the cli, once you've quit The above Dockerfile can be used as a template for creating best-practice nodejs Docker images in the future. For further help in crafting the best docker images possible for Node.js see Node.js Docker Best Practices . Knowing that it's Time to get go-ing fills you with determination . Part 3: Guide to Building Best-practice Go Docker Images \u00b6","title":"2. Learning How to Build Best-practice Node.js Docker Images"},{"location":"2-Best-Practice-Nodejs/#2-learning-how-to-build-best-practice-nodejs-docker-images","text":"First, we will go over the icp-node-js-sample app and Dockerfile. Then, we will see Dockerfile best practices for nodejs using the basic hello world app from the Node.js site. Note We will build all of these images in Part 4. This is a guide explaining the ins and outs of making the files to do so [i.e. the foundation for success]","title":"2. Learning How to Build Best-practice Node.js Docker Images"},{"location":"2-Best-Practice-Nodejs/#nodejs-download-for-later","text":"Here is the Node.js download if you want to run it locally to familiarize yourself with it/develop with it. For this guide, you actually don't need Node.js installed on your computer because of the magic of Docker. If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document.","title":"Node.js Download for Later"},{"location":"2-Best-Practice-Nodejs/#icp-nodejs-sample","text":"Here is the Dockerfile for the ICP Node.js Sample . Let's see what we got here folks: Run it with: docker run --rm -it -p 3000:3000 gmoney23/nodejs-sample Click on nodejs-sample (while it's running) to see it in your web browser. If on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. Here is what it will look like in the browser: Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell.","title":"ICP Node.js Sample"},{"location":"2-Best-Practice-Nodejs/#why-should-i-optimize-my-nodejs-images","text":"","title":"Why Should I Optimize my Nodejs Images?"},{"location":"2-Best-Practice-Nodejs/#security","text":"Running the container as root can potentially leave open security vulnerabilities which can be curtailed by running the container as a regular user. We will see how to do this in the example below. Additionally, smaller base images such as the alpine image below have a smaller attack surface in terms of vulnerable software (more software / packages = more potential vulnerabilities) and thus, regularly perform magnitudes better (have many times less vulnerabilities) in container security scans than base images of more traditional operating systems such as Ubuntu.","title":"Security"},{"location":"2-Best-Practice-Nodejs/#size","text":"By minimizing the size of images, we can prevent wasting resources for large base images and prevent using a ton of storage for all of our container images. Moreover, we can take better advantage of cache for our running containers.","title":"Size"},{"location":"2-Best-Practice-Nodejs/#nodejs-hello-world-server","text":"Here is the node-web-app Dockerfile where we can see comments for how to write a best practice Node.js Dockerfile. The simple code we're dockerizing for the web app comes from this Node.js guide Run it with: docker run --rm -it -p 8080:8080 gmoney23/node-web-app Click on node-web-app (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:8080 where serverip is your server's ip address. Here is what it will look like in the browser: Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. Here's what it will look like in the cli, once you've quit The above Dockerfile can be used as a template for creating best-practice nodejs Docker images in the future. For further help in crafting the best docker images possible for Node.js see Node.js Docker Best Practices . Knowing that it's Time to get go-ing fills you with determination .","title":"Node.js Hello World Server"},{"location":"2-Best-Practice-Nodejs/#part-3-guide-to-building-best-practice-go-docker-images","text":"","title":"Part 3: Guide to Building Best-practice Go Docker Images"},{"location":"3-Best-Practice-go/","text":"3. Guide to Building Best-practice Go Docker Images \u00b6 We will go over three apps and show how to optimize our Golang code for Docker. Go example Outyet program Go Hello world Go href-counter Note We will build all of these images in Part 4. This is a guide explaining the ins and outs of making the files to do so [i.e. the foundation for success]* But first...Download Go for later \u00b6 Here is the Go download if you want to run it locally to familiarize yourself with it/develop with it. For this section, you actually don't need golang installed on your computer because of the power of Docker. If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document. Outyet \u00b6 Outyet is a go example program from Go Outyet Example . This app checks if the version of Go specified (in our case 1.11 is out yet). Since we are using it, it better be! We will be dockerizing this app, shrinking our image down on the way over 3 iterations. Iteration 1: Outyet \u00b6 In this first iteration we make the go app in a container and get it running. Since there is a large base image needed for compiling the application and a large os is used for that image, this will be a rather large container. Here is the outyet Dockerfile . Read the comments below for details about the Dockerfile. Run it with: docker run --rm -it -p 3000:8080 gmoney23/outyet Note: This should just hang in the cli: The real action is in the browser. Click on outyet (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. It should look like this Here's the git page for go 1.11 when you click YES Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. To check it's size I ran: docker images gmoney23/outyet I got a whopping 786 Mb, no room for dessert :( Seeing the room for improvement fills you with determination . Iteration 2: Small-outyet \u00b6 In this second iteration, we attempt to improve upon our original endeavor using multi-stage builds. What is a multi-stage build? A build that happens in multiple stages. Mic drop... Docker multi-stage build . Employing multi-stage builds we can build the golang application in a container with all the bells and whistles and then copy it to another container that is much smaller. Here, we just run it. This works well with golang because when we set CGO_ENABLED=0 everything is statically compiled. In this case, we're going to copy it into the Alpine base image which should cut down its size considerably. Without further ado, the small-outyet Dockerfile below: Run it with docker run --rm -it -p 3000 :8080 gmoney23/small-outyet Click on small-outyet (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. Note: The cli hangs and the web page looks the same as for outyet , since it's the same app, so I've omitted images here. Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. To check it's size I ran: docker images gmoney23/small-outyet From 786MB -> 13.9MB that's some serious shrinkage. The amount the container has shrunk fills you with determination . Iteration 3: Smallest-Outyet \u00b6 How do we get smaller than starting with a 5MB alpine image? How about start with nothing. We are going to use the special scratch image which starts fresh. Since everything can be set to statically compile in go with CGO_ENABLED=0 , we can just package the binary in a container without even a shell. This lessons attack surface and gives us a super light image. On top of that, we'll add some compiler flags for production to cut off the debug info space in go. Here's how it all looks in the smallest-outyet Dockerfile Run it: docker run --rm -it -p 3000 :8080 gmoney23/smallest-outyet Click on smallest-outyet (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. Note: You know the drill. The cli hangs and the web page looks the same as for outyet and small-outyet , since it's the same app, so I've omitted images here. Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. To check it's size I ran: docker images gmoney23/smallest-outyet From 13.9MB -> 6.89MB for a grand transformation of 786MB -> 6.89MB, a little over 114X smaller than the original image! That's a lot of room for dessert :) Why was our Go Container Transformation Worth it? \u00b6 Security \u00b6 Running the container from scratch enables us to take away even a shell into our container. The only software we are adding to the container is the executable that needs to run. This dramatically reduces the attack surface and means you don't need to worry about the security patches for the libs/bins you are adding in your container (since you aren't adding any). You simply need to worry about the security of your application itself and the pieces it is made with. Size \u00b6 By minimizing the size of images, we can prevent wasting resources for large base images and prevent using a ton of storage for all of our container images. Moreover, we can take better advantage of cache for our running containers. Clarity \u00b6 It is easy to see exactly what we are doing in our container and we don't need to worry about potential problems when updating the bins/libs of our container because we don't have any. If the scratch image has piqued your interest, check out this article exploring \"Inside Docker's FROM scratch\" Go Hello World \u00b6 Using the techniques we just employed, let's see how small of a Docker image we can make for a basic go hello world app from gist. Say hello to the example-go-server Dockerfile . Run it with: docker run --rm -it -p 3000:5000 gmoney23/example-go-server Click on example-go-server (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. Here is what it will look like in the browser: Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. Here's what it will look like in the cli after its been shut down. To check its size I ran: docker images gmoney23/example-go-server This gives us an image size of 4.9MB, quite astounding! Href-Counter \u00b6 Finally, lets dockerize an app that prints output for us instead of a web app. Href-counter is an application that counts the number of internal and external-hrefs on a web-page to rate SEO. It is referenced in the multi-stage build manual for docker we looked at before and fits the bill for us. Let's take a peak at the href-counter Dockerfile inside of MultiArchDockerKubernetes. Note: This tool is operating on live websites so numbers will change as sites are updated, altering their internal and external hrefs \u00b6 We can try the tool out against different sites (starting with this site) using: docker run --rm -e url=https://siler23.github.io/MultiArchDockerKubernetes/ gmoney23/href {\"internal\":35,\"external\":9} Note For PROXY**: add your -e for http_proxy, etc.: docker run --rm -e http_proxy= %http_proxy% -e https_proxy= %https_proxy% -e no_proxy= \" %no_proxy% \" -e url=http://google.com href More Examples docker run --rm -e url = http://yahoo.com gmoney23/href {\"internal\":29,\"external\":82} docker run --rm -e url = http://blog.alexellis.io/ gmoney23/href {\"internal\":51,\"external\":2} You'll be pleased to know our Dockerfile made this image small as well. We can see with: docker images gmoney23/href For more go best practices and tips with Docker see this excellent article Part 4: Bringing Multi-arch Images to a Computer Near You! \u00b6","title":"3. Guide to Building Best-practice Go Docker Images"},{"location":"3-Best-Practice-go/#3-guide-to-building-best-practice-go-docker-images","text":"We will go over three apps and show how to optimize our Golang code for Docker. Go example Outyet program Go Hello world Go href-counter Note We will build all of these images in Part 4. This is a guide explaining the ins and outs of making the files to do so [i.e. the foundation for success]*","title":"3. Guide to Building Best-practice Go Docker Images"},{"location":"3-Best-Practice-go/#but-firstdownload-go-for-later","text":"Here is the Go download if you want to run it locally to familiarize yourself with it/develop with it. For this section, you actually don't need golang installed on your computer because of the power of Docker. If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document.","title":"But first...Download Go for later"},{"location":"3-Best-Practice-go/#outyet","text":"Outyet is a go example program from Go Outyet Example . This app checks if the version of Go specified (in our case 1.11 is out yet). Since we are using it, it better be! We will be dockerizing this app, shrinking our image down on the way over 3 iterations.","title":"Outyet"},{"location":"3-Best-Practice-go/#iteration-1-outyet","text":"In this first iteration we make the go app in a container and get it running. Since there is a large base image needed for compiling the application and a large os is used for that image, this will be a rather large container. Here is the outyet Dockerfile . Read the comments below for details about the Dockerfile. Run it with: docker run --rm -it -p 3000:8080 gmoney23/outyet Note: This should just hang in the cli: The real action is in the browser. Click on outyet (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. It should look like this Here's the git page for go 1.11 when you click YES Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. To check it's size I ran: docker images gmoney23/outyet I got a whopping 786 Mb, no room for dessert :( Seeing the room for improvement fills you with determination .","title":"Iteration 1: Outyet"},{"location":"3-Best-Practice-go/#iteration-2-small-outyet","text":"In this second iteration, we attempt to improve upon our original endeavor using multi-stage builds. What is a multi-stage build? A build that happens in multiple stages. Mic drop... Docker multi-stage build . Employing multi-stage builds we can build the golang application in a container with all the bells and whistles and then copy it to another container that is much smaller. Here, we just run it. This works well with golang because when we set CGO_ENABLED=0 everything is statically compiled. In this case, we're going to copy it into the Alpine base image which should cut down its size considerably. Without further ado, the small-outyet Dockerfile below: Run it with docker run --rm -it -p 3000 :8080 gmoney23/small-outyet Click on small-outyet (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. Note: The cli hangs and the web page looks the same as for outyet , since it's the same app, so I've omitted images here. Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. To check it's size I ran: docker images gmoney23/small-outyet From 786MB -> 13.9MB that's some serious shrinkage. The amount the container has shrunk fills you with determination .","title":"Iteration 2: Small-outyet"},{"location":"3-Best-Practice-go/#iteration-3-smallest-outyet","text":"How do we get smaller than starting with a 5MB alpine image? How about start with nothing. We are going to use the special scratch image which starts fresh. Since everything can be set to statically compile in go with CGO_ENABLED=0 , we can just package the binary in a container without even a shell. This lessons attack surface and gives us a super light image. On top of that, we'll add some compiler flags for production to cut off the debug info space in go. Here's how it all looks in the smallest-outyet Dockerfile Run it: docker run --rm -it -p 3000 :8080 gmoney23/smallest-outyet Click on smallest-outyet (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. Note: You know the drill. The cli hangs and the web page looks the same as for outyet and small-outyet , since it's the same app, so I've omitted images here. Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. To check it's size I ran: docker images gmoney23/smallest-outyet From 13.9MB -> 6.89MB for a grand transformation of 786MB -> 6.89MB, a little over 114X smaller than the original image! That's a lot of room for dessert :)","title":"Iteration 3: Smallest-Outyet"},{"location":"3-Best-Practice-go/#why-was-our-go-container-transformation-worth-it","text":"","title":"Why was our Go Container Transformation Worth it?"},{"location":"3-Best-Practice-go/#security","text":"Running the container from scratch enables us to take away even a shell into our container. The only software we are adding to the container is the executable that needs to run. This dramatically reduces the attack surface and means you don't need to worry about the security patches for the libs/bins you are adding in your container (since you aren't adding any). You simply need to worry about the security of your application itself and the pieces it is made with.","title":"Security"},{"location":"3-Best-Practice-go/#size","text":"By minimizing the size of images, we can prevent wasting resources for large base images and prevent using a ton of storage for all of our container images. Moreover, we can take better advantage of cache for our running containers.","title":"Size"},{"location":"3-Best-Practice-go/#clarity","text":"It is easy to see exactly what we are doing in our container and we don't need to worry about potential problems when updating the bins/libs of our container because we don't have any. If the scratch image has piqued your interest, check out this article exploring \"Inside Docker's FROM scratch\"","title":"Clarity"},{"location":"3-Best-Practice-go/#go-hello-world","text":"Using the techniques we just employed, let's see how small of a Docker image we can make for a basic go hello world app from gist. Say hello to the example-go-server Dockerfile . Run it with: docker run --rm -it -p 3000:5000 gmoney23/example-go-server Click on example-go-server (while it's running) to see it in your web browser. If you're on a server instead of a desktop go to http://serverip:3000 where serverip is your server's ip address. Here is what it will look like in the browser: Tip Quit the app by hitting both the control and c keys (ctrl c) in the terminal/ command prompt / PowerShell. Here's what it will look like in the cli after its been shut down. To check its size I ran: docker images gmoney23/example-go-server This gives us an image size of 4.9MB, quite astounding!","title":"Go Hello World"},{"location":"3-Best-Practice-go/#href-counter","text":"Finally, lets dockerize an app that prints output for us instead of a web app. Href-counter is an application that counts the number of internal and external-hrefs on a web-page to rate SEO. It is referenced in the multi-stage build manual for docker we looked at before and fits the bill for us. Let's take a peak at the href-counter Dockerfile inside of MultiArchDockerKubernetes.","title":"Href-Counter"},{"location":"3-Best-Practice-go/#note-this-tool-is-operating-on-live-websites-so-numbers-will-change-as-sites-are-updated-altering-their-internal-and-external-hrefs","text":"We can try the tool out against different sites (starting with this site) using: docker run --rm -e url=https://siler23.github.io/MultiArchDockerKubernetes/ gmoney23/href {\"internal\":35,\"external\":9} Note For PROXY**: add your -e for http_proxy, etc.: docker run --rm -e http_proxy= %http_proxy% -e https_proxy= %https_proxy% -e no_proxy= \" %no_proxy% \" -e url=http://google.com href More Examples docker run --rm -e url = http://yahoo.com gmoney23/href {\"internal\":29,\"external\":82} docker run --rm -e url = http://blog.alexellis.io/ gmoney23/href {\"internal\":51,\"external\":2} You'll be pleased to know our Dockerfile made this image small as well. We can see with: docker images gmoney23/href For more go best practices and tips with Docker see this excellent article","title":"Note: This tool is operating on live websites so numbers will change as sites are updated, altering their internal and external hrefs"},{"location":"3-Best-Practice-go/#part-4-bringing-multi-arch-images-to-a-computer-near-you","text":"","title":"Part 4: Bringing Multi-arch Images to a Computer Near You!"},{"location":"4-Build-MultiArch/","text":"4. Bringing Multi-arch Images to a Computer Near You \u00b6 This section goes through installing the manifest tool and building the multi-arch docker images. Don't forget to set the multi-arch lab home directory \u00b6 export MULTIARCH_HOME = \"full path to directory of multiarch repo\" If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document. Enabling Docker Experimental Features \u00b6 We need to enable experimental features for both the docker client and server in order to use the docker manifest command and --platform tags respectively. In order to do this please follow the steps below for your operating system. docker manifest enables us to push and pull manifest lists while --platform gives us the ability to pull images of a specific platform (i.e. operating system/architecture). This lets us pull s390x images even if we are on an amd64 platform which is needed to get the base images for our cross-builds later. Attention Please use Docker 18.06 or later . If you don't have at least this version, please upgrade. Check with: docker version Under Client: If Experimental: false like in the above picture then you need to do the Client steps. If Experimental: true then you can skip the Client steps. Under Server: If Experimental: false like in the above picture, you need to do the Server steps. If Experimental: true then you can skip the Server steps. Mac / Windows Open the Preferences from the menu Client \u00b6 Go to Command Line and click to Enable experimental features. Then, click Apply and Restart . Server \u00b6 Go to Docker Engine and change false to true for experimental. Then, click Apply & Restart . Check for Success \u00b6 Once docker is started check docker version again to see experimental set to true for both client and server: docker version Linux Client \u00b6 ls \"$HOME/.docker\" If $HOME/.docker/config.json exists: (Open config.json with your favorite text editor) vim $HOME/.docker/config.json Add \"experimental\": \"enabled\" to config.json file If $HOME/.docker/config.json doesn\u2019t exist: mkdir \"$HOME/.docker\" echo $'{\\n \"experimental\": \"enabled\"\\n}' | tee $HOME/.docker/config.json Add \"experimental\": \"enabled\" to config.json file Server \u00b6 sudo ls /etc/docker If /etc/docker/daemon.json exists: \u00b6 (Open daemon.json with your favorite text editor) sudo vim /etc/docker/daemon.json Add \"experimental\": true to daemon.json file If /etc/docker/daemon.json doesn't exist: \u00b6 echo $'{\\n \"experimental\": true\\n}' | sudo tee /etc/docker/daemon.json Restart Docker to Pick Up Changes \u00b6 sudo service docker restart Check for Success \u00b6 Once docker is started, check docker version again to see experimental set to true for both client and server: docker version Cross-Architecture Docker \u00b6 Normally, one can only run docker images compiled for the host machine's architecture. This means that in order to run an s390x image, you would need an s390x server. Additionally, since building an s390x image (in most cases) requires running s390x binaries on your system, this also requires an s390x server. The same holds true for arm, x86 (amd64), power (ppc64le), etc. This limits the ability to build images that are available across all platforms. One way to overcome this limitation is by using binfmt_misc in conjunction with qemu (quick emulation) running using user-mode-emulation . Qemu dynamically translates the target architecture's instructions to the the host architecture's instruction set to enable binaries of a different architecture to run on a host system. Binfmt_misc comes in to enable the kernel to read the foreign architecture binary by \"directing\" the kernel to the correct qemu static binary to interpret the code. In order to set this up to work with Docker and its underlying linux kernel, we first need: binfmt_misc to register the corresponding qemu-static binary for each architecture that has a static qemu image available [of course other than the native one]. provide these qemu-static binaries to the system for the host architecture to all of the target architectures. You can get these by building them from the qemu code or from a package of qemu for your os. To get the most recent ones, it is best to build from source at a stable version of the code load the static immediately so it is in place to be used with Docker for each container it creates in their new sets of namespaces [read: use F flag for binfmt_misc setup] Setting up Cross-Architecture support \u00b6 The docker/binfmt GitHub project creates a docker image which completes all 3 tasks. Their implementation works for an amd64 host, so I made a separate image with the qemu-static binaries compiled from the s390x host and posted a multi-arch image for both amd64 and s390x hosts to gmoney23/binfmt . You can test this out by first running an image that is from a different platform than yours. Mac / Windows Docker for Mac and Docker for Windows have this capability built-in out of the box so we don't even have to set it up with an image run. However, don't just take my word for it, demonstrate it to yourself by trying the s390x (z) image for hello-world on your amd64 (x86) mac: docker pull --platform s390x hello-world Confirm the architecture with: uname -m Confirm the image architecture with: docker inspect -f '{{.Architecture}}' hello-world Finally, run the image successfully the 1 st try with: docker run hello-world This works successfully (without additional configuration needed) What do you mean by this capability is built-in out of the box? Docker for Mac and Docker for Windows run containers in a vm (either a hypervisor of hyperkit on mac [a derivative of xhyve] or hyper-v on windows) which it uses to run a linux kernel that in this case is set up with binfmt_misc which is used to emulate the other architectures by setting up a file system with a static arch interpreter (a qemu linux-user static file for the given arch). The setup we did for linux, the Docker install does for us on mac/windows as part of the setup for that environment. Meanwhile, since we already have a linux kernel with docker for linux, we get the privilege of making our own changes to it for now. If this brief talk about Docker for Mac internals piqued your interest, I highly encourage Under the Hood: Demystifying Docker For Mac CE Edition The ease of running and building multi-arch on mac and windows fills you with determination . Linux Run the docker pull with --platform for the architecture different from yours. amd64 host (most users) docker pull --platform s390x hello-world Then, run the pulled image docker run hello-world s390x host (a few potential users) docker pull --platform amd64 hello-world Then, run the pulled image docker run hello-world Note While the pictures in this section are for an s390x host, if you are using an amd64 host you should be getting similar errors with the s390x image you pulled. No need to worry that some of the messages say s390x for you vs amd64 in the pictures that follow in this section. You should get an exec format error like so: Now, run the docker image to perform the aforementioned required 3 steps on either s390x of amd64: docker run --rm --privileged gmoney23/binfmt --privileged gives the container the necessary permissions it requires. The script within this container registers qemu linux-user binaries with binfmt_misc. (These binaries are statically compiled on the host architecture for each supported target architecture. In our case: s390x, arm, and ppc64le on amd64 and arm, amd64, and ppc64le on s390x). These binaries are generated during container build and are found within the gmoney23/binfmt container. They are registered with the F (fixed) flag which opens the static binary as soon as it is installed so that spawned containers can use the qemu static. This support was added to the linux kernel to enable things such as the cross-building we are going to do. (The full options are OCF and you can see the full description for each option here ) Confirm the architecture with: uname -m Confirm the image architecture with: docker inspect -f '{{.Architecture}}' hello-world Finally, run the image successfully with the same command as before: docker run hello-world We can also see in the hello-world output the amd64 architecture mentioned. Thus we have achieved our goal of running amd64 (x86) images on s390x . Consequences of Cross-Architecture Docker \u00b6 This enables us to not only run images, but also build them for different architectures from a given host architecture. This makes it possible to build s390x (z), power (ppc64le), arm, and amd64 images all from the hosts you have available. This enables developers to support more images as they may have nodes with only specific architectures such as amd64 . Using this technology, suddenly ecosystem contribution is no longer constrained by host architecture limitations, creating a broader docker image ecosystem for everyone. In fact, with the current stable docker CE build and onward buildx comes as an experimental future to build and push multi-arch images (using qemu behind the scenes) in a seamless process which we will briefly explore in a recommended optional follow-up to this section (given you have a workstation with Docker CE of version 19.0.3 or later). The big caveat to this capability remains that qemu does not support all instructions and is being constantly improved to handle more exceptions and work for more use cases in the future. What is supported will be dictated by which linux-user static binary you are using with arm and s390x having more support than ppc64le at the current time (at least for nodejs). This means for certain image builds you will still have to use the native hardware, but for many images qemu provides a quick and easy way to build images for more platforms. Next, we will use an amd64 host to build images for both architectures and use them to make multi-arch images that support both architectures for each of the applications we have visited in parts 2 and 3 of this tutorial. Note: amd64 linux-user does not have the capability required for these images at this time (i.e. running npm install) which is why the host needs to be amd64 if we want to include an amd64 image. This is largely in part to lack of need to emulate amd64 due to its wide availability and most personal workstations using amd64 architecture. Making multi-arch docker images \u00b6 In order to build all of the images for amd64 (x86), s390x (z) architectures, and power (ppc64le) architectures we will use a simple script that I wrote to go through the steps of building each individual architecture image with both a versioned and latest tag. Then, it creates a manifest list for each application and pushes it up to form a multiarch image for each of the applications we have gone over. It is heavily commented so it should explain itself. Note: It has been updated to just create one smallest-outyet image currently by adding a new line IMAGES=(\"smallest-outyet\") to override the full images list. You can simply uncomment this line to build all images but that will take a long time so this is a time save as the actual building is trivial (it is the repeatable process [which could easily be applied to all images as in this scenario] that is important). If you want to visit the script itself to see it up close and in a bigger font, please click on Build and Push Images Script Login to your Docker Repo [Account] \u00b6 Set DOCKER_REPO=<docker_username> which for me is: DOCKER_REPO = gmoney23 Set yours accordingly, then do a docker login: docker login -u ${ DOCKER_REPO } Enter your password when prompted: Run Script to build and Push Images \u00b6 Change into the main directory where you cloned or downloaded the github repository with the variable MULTIARCH_HOME you set at the start of this section to be the full path to the github project's directory on your computer. cd \"${MULTIARCH_HOME}\"/ Without Proxy \u00b6 The options are as follows: DOCKER_REPO=<my_docker_repo> VERSION=<version_number> IMAGE_PREPEND=<prepend_to_make_unique_image> LATEST=<true_or_false> ./Build_And_Push_Images.sh Sample Command with my docker repo of gmoney23 please replace with your docker repo (the one you used for docker login above): DOCKER_REPO=gmoney23 DOCKER_REPO=${DOCKER_REPO} VERSION=1.0 IMAGE_PREPEND=marchdockerlab LATEST=true ./Build_And_Push_Images.sh With Proxy \u00b6 The options are as follows: DOCKER_REPO=<my_docker_repo> VERSION=<version_number> IMAGE_PREPEND=<prepend_to_make_unique_image> LATEST=<true_or_false> http_proxy=<proxy_for_http_protocol> https_proxy=<proxy_for_https_protocol> no_proxy=<addresses_not_to_be_proxied> ./Build_And_Push_Images.sh Sample Command with my docker repo of gmoney23 please replace with your docker repo (the one you used for docker login above): DOCKER_REPO=gmoney23 DOCKER_REPO=${DOCKER_REPO} VERSION=1.0 IMAGE_PREPEND=marchdockerlab LATEST=true http_proxy=http://myproxy:8080 https_proxy=http://myproxy:8080 no_proxy=\"localhost, 127.0.0.1\" ./Build_And_Push_Images.sh Knowing that qemu builds a bridge from x to z for free fills you with determination . OPTIONAL SECTIONS \u00b6 Recommended: [Most users should try if they have docker 19.03 CE or later]: The future - Using Buildx to make Multi-arch a Way of Life \u00b6 Extra Content: [Most Users Should Skip]: The past - The Manual Way aka Herding Cats \u00b6 This optional path is a manual collection of tasks to build images in more depth if you want more detail. This is purely for educational purposes if something in the script didn't make sense or you want further material and not part of the main path. Part 5: Kubernetes Time \u00b6","title":"Bringing Multi-arch Images to a Computer Near You"},{"location":"4-Build-MultiArch/#4-bringing-multi-arch-images-to-a-computer-near-you","text":"This section goes through installing the manifest tool and building the multi-arch docker images.","title":"4. Bringing Multi-arch Images to a Computer Near You"},{"location":"4-Build-MultiArch/#dont-forget-to-set-the-multi-arch-lab-home-directory","text":"export MULTIARCH_HOME = \"full path to directory of multiarch repo\" If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document.","title":"Don't forget to set the multi-arch lab home directory"},{"location":"4-Build-MultiArch/#enabling-docker-experimental-features","text":"We need to enable experimental features for both the docker client and server in order to use the docker manifest command and --platform tags respectively. In order to do this please follow the steps below for your operating system. docker manifest enables us to push and pull manifest lists while --platform gives us the ability to pull images of a specific platform (i.e. operating system/architecture). This lets us pull s390x images even if we are on an amd64 platform which is needed to get the base images for our cross-builds later. Attention Please use Docker 18.06 or later . If you don't have at least this version, please upgrade. Check with: docker version Under Client: If Experimental: false like in the above picture then you need to do the Client steps. If Experimental: true then you can skip the Client steps. Under Server: If Experimental: false like in the above picture, you need to do the Server steps. If Experimental: true then you can skip the Server steps. Mac / Windows Open the Preferences from the menu","title":"Enabling Docker Experimental Features"},{"location":"4-Build-MultiArch/#client","text":"Go to Command Line and click to Enable experimental features. Then, click Apply and Restart .","title":"Client"},{"location":"4-Build-MultiArch/#server","text":"Go to Docker Engine and change false to true for experimental. Then, click Apply & Restart .","title":"Server"},{"location":"4-Build-MultiArch/#check-for-success","text":"Once docker is started check docker version again to see experimental set to true for both client and server: docker version Linux","title":"Check for Success"},{"location":"4-Build-MultiArch/#client_1","text":"ls \"$HOME/.docker\" If $HOME/.docker/config.json exists: (Open config.json with your favorite text editor) vim $HOME/.docker/config.json Add \"experimental\": \"enabled\" to config.json file If $HOME/.docker/config.json doesn\u2019t exist: mkdir \"$HOME/.docker\" echo $'{\\n \"experimental\": \"enabled\"\\n}' | tee $HOME/.docker/config.json Add \"experimental\": \"enabled\" to config.json file","title":"Client"},{"location":"4-Build-MultiArch/#server_1","text":"sudo ls /etc/docker","title":"Server"},{"location":"4-Build-MultiArch/#if-etcdockerdaemonjson-exists","text":"(Open daemon.json with your favorite text editor) sudo vim /etc/docker/daemon.json Add \"experimental\": true to daemon.json file","title":"If /etc/docker/daemon.json exists:"},{"location":"4-Build-MultiArch/#if-etcdockerdaemonjson-doesnt-exist","text":"echo $'{\\n \"experimental\": true\\n}' | sudo tee /etc/docker/daemon.json","title":"If /etc/docker/daemon.json doesn't exist:"},{"location":"4-Build-MultiArch/#restart-docker-to-pick-up-changes","text":"sudo service docker restart","title":"Restart Docker to Pick Up Changes"},{"location":"4-Build-MultiArch/#check-for-success_1","text":"Once docker is started, check docker version again to see experimental set to true for both client and server: docker version","title":"Check for Success"},{"location":"4-Build-MultiArch/#cross-architecture-docker","text":"Normally, one can only run docker images compiled for the host machine's architecture. This means that in order to run an s390x image, you would need an s390x server. Additionally, since building an s390x image (in most cases) requires running s390x binaries on your system, this also requires an s390x server. The same holds true for arm, x86 (amd64), power (ppc64le), etc. This limits the ability to build images that are available across all platforms. One way to overcome this limitation is by using binfmt_misc in conjunction with qemu (quick emulation) running using user-mode-emulation . Qemu dynamically translates the target architecture's instructions to the the host architecture's instruction set to enable binaries of a different architecture to run on a host system. Binfmt_misc comes in to enable the kernel to read the foreign architecture binary by \"directing\" the kernel to the correct qemu static binary to interpret the code. In order to set this up to work with Docker and its underlying linux kernel, we first need: binfmt_misc to register the corresponding qemu-static binary for each architecture that has a static qemu image available [of course other than the native one]. provide these qemu-static binaries to the system for the host architecture to all of the target architectures. You can get these by building them from the qemu code or from a package of qemu for your os. To get the most recent ones, it is best to build from source at a stable version of the code load the static immediately so it is in place to be used with Docker for each container it creates in their new sets of namespaces [read: use F flag for binfmt_misc setup]","title":"Cross-Architecture Docker"},{"location":"4-Build-MultiArch/#setting-up-cross-architecture-support","text":"The docker/binfmt GitHub project creates a docker image which completes all 3 tasks. Their implementation works for an amd64 host, so I made a separate image with the qemu-static binaries compiled from the s390x host and posted a multi-arch image for both amd64 and s390x hosts to gmoney23/binfmt . You can test this out by first running an image that is from a different platform than yours. Mac / Windows Docker for Mac and Docker for Windows have this capability built-in out of the box so we don't even have to set it up with an image run. However, don't just take my word for it, demonstrate it to yourself by trying the s390x (z) image for hello-world on your amd64 (x86) mac: docker pull --platform s390x hello-world Confirm the architecture with: uname -m Confirm the image architecture with: docker inspect -f '{{.Architecture}}' hello-world Finally, run the image successfully the 1 st try with: docker run hello-world This works successfully (without additional configuration needed) What do you mean by this capability is built-in out of the box? Docker for Mac and Docker for Windows run containers in a vm (either a hypervisor of hyperkit on mac [a derivative of xhyve] or hyper-v on windows) which it uses to run a linux kernel that in this case is set up with binfmt_misc which is used to emulate the other architectures by setting up a file system with a static arch interpreter (a qemu linux-user static file for the given arch). The setup we did for linux, the Docker install does for us on mac/windows as part of the setup for that environment. Meanwhile, since we already have a linux kernel with docker for linux, we get the privilege of making our own changes to it for now. If this brief talk about Docker for Mac internals piqued your interest, I highly encourage Under the Hood: Demystifying Docker For Mac CE Edition The ease of running and building multi-arch on mac and windows fills you with determination . Linux Run the docker pull with --platform for the architecture different from yours. amd64 host (most users) docker pull --platform s390x hello-world Then, run the pulled image docker run hello-world s390x host (a few potential users) docker pull --platform amd64 hello-world Then, run the pulled image docker run hello-world Note While the pictures in this section are for an s390x host, if you are using an amd64 host you should be getting similar errors with the s390x image you pulled. No need to worry that some of the messages say s390x for you vs amd64 in the pictures that follow in this section. You should get an exec format error like so: Now, run the docker image to perform the aforementioned required 3 steps on either s390x of amd64: docker run --rm --privileged gmoney23/binfmt --privileged gives the container the necessary permissions it requires. The script within this container registers qemu linux-user binaries with binfmt_misc. (These binaries are statically compiled on the host architecture for each supported target architecture. In our case: s390x, arm, and ppc64le on amd64 and arm, amd64, and ppc64le on s390x). These binaries are generated during container build and are found within the gmoney23/binfmt container. They are registered with the F (fixed) flag which opens the static binary as soon as it is installed so that spawned containers can use the qemu static. This support was added to the linux kernel to enable things such as the cross-building we are going to do. (The full options are OCF and you can see the full description for each option here ) Confirm the architecture with: uname -m Confirm the image architecture with: docker inspect -f '{{.Architecture}}' hello-world Finally, run the image successfully with the same command as before: docker run hello-world We can also see in the hello-world output the amd64 architecture mentioned. Thus we have achieved our goal of running amd64 (x86) images on s390x .","title":"Setting up Cross-Architecture support"},{"location":"4-Build-MultiArch/#consequences-of-cross-architecture-docker","text":"This enables us to not only run images, but also build them for different architectures from a given host architecture. This makes it possible to build s390x (z), power (ppc64le), arm, and amd64 images all from the hosts you have available. This enables developers to support more images as they may have nodes with only specific architectures such as amd64 . Using this technology, suddenly ecosystem contribution is no longer constrained by host architecture limitations, creating a broader docker image ecosystem for everyone. In fact, with the current stable docker CE build and onward buildx comes as an experimental future to build and push multi-arch images (using qemu behind the scenes) in a seamless process which we will briefly explore in a recommended optional follow-up to this section (given you have a workstation with Docker CE of version 19.0.3 or later). The big caveat to this capability remains that qemu does not support all instructions and is being constantly improved to handle more exceptions and work for more use cases in the future. What is supported will be dictated by which linux-user static binary you are using with arm and s390x having more support than ppc64le at the current time (at least for nodejs). This means for certain image builds you will still have to use the native hardware, but for many images qemu provides a quick and easy way to build images for more platforms. Next, we will use an amd64 host to build images for both architectures and use them to make multi-arch images that support both architectures for each of the applications we have visited in parts 2 and 3 of this tutorial. Note: amd64 linux-user does not have the capability required for these images at this time (i.e. running npm install) which is why the host needs to be amd64 if we want to include an amd64 image. This is largely in part to lack of need to emulate amd64 due to its wide availability and most personal workstations using amd64 architecture.","title":"Consequences of Cross-Architecture Docker"},{"location":"4-Build-MultiArch/#making-multi-arch-docker-images","text":"In order to build all of the images for amd64 (x86), s390x (z) architectures, and power (ppc64le) architectures we will use a simple script that I wrote to go through the steps of building each individual architecture image with both a versioned and latest tag. Then, it creates a manifest list for each application and pushes it up to form a multiarch image for each of the applications we have gone over. It is heavily commented so it should explain itself. Note: It has been updated to just create one smallest-outyet image currently by adding a new line IMAGES=(\"smallest-outyet\") to override the full images list. You can simply uncomment this line to build all images but that will take a long time so this is a time save as the actual building is trivial (it is the repeatable process [which could easily be applied to all images as in this scenario] that is important). If you want to visit the script itself to see it up close and in a bigger font, please click on Build and Push Images Script","title":"Making multi-arch docker images"},{"location":"4-Build-MultiArch/#login-to-your-docker-repo-account","text":"Set DOCKER_REPO=<docker_username> which for me is: DOCKER_REPO = gmoney23 Set yours accordingly, then do a docker login: docker login -u ${ DOCKER_REPO } Enter your password when prompted:","title":"Login to your Docker Repo [Account]"},{"location":"4-Build-MultiArch/#run-script-to-build-and-push-images","text":"Change into the main directory where you cloned or downloaded the github repository with the variable MULTIARCH_HOME you set at the start of this section to be the full path to the github project's directory on your computer. cd \"${MULTIARCH_HOME}\"/","title":"Run Script to build and Push Images"},{"location":"4-Build-MultiArch/#without-proxy","text":"The options are as follows: DOCKER_REPO=<my_docker_repo> VERSION=<version_number> IMAGE_PREPEND=<prepend_to_make_unique_image> LATEST=<true_or_false> ./Build_And_Push_Images.sh Sample Command with my docker repo of gmoney23 please replace with your docker repo (the one you used for docker login above): DOCKER_REPO=gmoney23 DOCKER_REPO=${DOCKER_REPO} VERSION=1.0 IMAGE_PREPEND=marchdockerlab LATEST=true ./Build_And_Push_Images.sh","title":"Without Proxy"},{"location":"4-Build-MultiArch/#with-proxy","text":"The options are as follows: DOCKER_REPO=<my_docker_repo> VERSION=<version_number> IMAGE_PREPEND=<prepend_to_make_unique_image> LATEST=<true_or_false> http_proxy=<proxy_for_http_protocol> https_proxy=<proxy_for_https_protocol> no_proxy=<addresses_not_to_be_proxied> ./Build_And_Push_Images.sh Sample Command with my docker repo of gmoney23 please replace with your docker repo (the one you used for docker login above): DOCKER_REPO=gmoney23 DOCKER_REPO=${DOCKER_REPO} VERSION=1.0 IMAGE_PREPEND=marchdockerlab LATEST=true http_proxy=http://myproxy:8080 https_proxy=http://myproxy:8080 no_proxy=\"localhost, 127.0.0.1\" ./Build_And_Push_Images.sh Knowing that qemu builds a bridge from x to z for free fills you with determination .","title":"With Proxy"},{"location":"4-Build-MultiArch/#optional-sections","text":"","title":"OPTIONAL SECTIONS"},{"location":"4-Build-MultiArch/#recommended-most-users-should-try-if-they-have-docker-1903-ce-or-later-the-future-using-buildx-to-make-multi-arch-a-way-of-life","text":"","title":"Recommended: [Most users should try if they have docker 19.03 CE or later]: The future - Using Buildx to make Multi-arch a Way of Life"},{"location":"4-Build-MultiArch/#extra-content-most-users-should-skip-the-past-the-manual-way-aka-herding-cats","text":"This optional path is a manual collection of tasks to build images in more depth if you want more detail. This is purely for educational purposes if something in the script didn't make sense or you want further material and not part of the main path.","title":"Extra Content: [Most Users Should Skip]: The past - The Manual Way aka Herding Cats"},{"location":"4-Build-MultiArch/#part-5-kubernetes-time","text":"","title":"Part 5: Kubernetes Time"},{"location":"4-Multiarch-buildx-addendum/","text":"The future - Using Buildx to make Multi-arch a Way of Life \u00b6 If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document. Buildx: Seamless multi-arch builds are in your future \u00b6 Buildx has become a part of stable Docker builds with Docker CE 19.03 as an experimental feature. Since we have already enabled experimental features (if our docker version is 19.03 or greater) [if you don't remember run docker version ], we are ready to use buildx. We have already built our images above using a script. Let's see what the future holds... Setup Buildx \u00b6 First, we will create a new builder: docker buildx create --name multi-arch Next, we will set this to our active builder: docker buildx use multi-arch Then, we will bootstrap it. At this point it will look for the supported architectures on our system using qemu and list them for us. docker buildx inspect --bootstrap Finally, we can see our current builders with: docker buildx ls Here is how this all looks from my mac: Notice, that the 2 architectures we want to use (s390x and amd64) are both supported on my system. This is because the qemu emulation is set up for s390x, arm (multiple versions), and ppc64le and my host platform is amd64 (in this case). This will work on any machine once its set up: either out-of-the-box with docker desktop for mac or windows or after you enable qemu on your linux box like we did above with the gmoney23/binfmt image. Login to our Docker Repo \u00b6 Set DOCKER_REPO=<docker_username> which for me is: DOCKER_REPO=gmoney23 Set yours accordingly, then do a docker login: docker login -u ${DOCKER_REPO} Use Buildx to create multi-arch images \u00b6 Now, we can create a multi-architecture image for the three (x, z, and power) architectures (and more if desired) in one command: docker buildx build --platform linux/amd64,linux/s390x,linux/ppc64le -t ${DOCKER_REPO}/buildx-hello-node:1.0 \"${MULTIARCH_HOME}\"/node-web-app --push Info We are using the DOCKER_REPO we just logged in to with the environment variable we just set above* We can look at our newly pushed image to verify with: docker run mplatform/mquery ${DOCKER_REPO}/buildx-hello-node:1.0 To see the full manifest list we can inspect it with: docker manifest inspect ${DOCKER_REPO}/buildx-hello-node:1.0 Buildx isn't just limited to multi-arch builds with statics, it can also register remote builders to build images on those machines. To do this you will want to define multiple docker contexts which you can use to define multiple machines as detailed here . Then you can target these with your buildx builders. Again, if you want to check it out see the buildx github . Also, if you want to see a walk-through on buildx with arm see Building Multi-Arch Images for Arm and x86 with Docker Desktop . Additionally, in 2020 a wild walk-through on buildx appeared . It is worth checking out if you need a little more detail. We can use buildx to simplify the multi-arch process even further by bringing our build pipeline together in one place and allowing us to concurrently construct images for all arches that we can use in our manifest lists to \"create\" multi-arch images in one command. Now, it's time to use the multi-arch images in Kubernetes! OPTIONAL SECTION \u00b6 Extra Content: [Most Users Should Skip]: The past - The Manual Way aka Herding Cats \u00b6 Note This optional path is a manual collection of tasks to build images in more depth if you want more detail. This is purely for educational purposes if something in the script didn't make sense or you want further material and not part of the main path. Summary \u00b6 The knowledge that you can build and push multiarch images with one command is splendid, it fills you with determination Part 5: Kubernetes Time \u00b6","title":"The future - Using Buildx to make Multi-arch a Way of Life"},{"location":"4-Multiarch-buildx-addendum/#the-future-using-buildx-to-make-multi-arch-a-way-of-life","text":"If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document.","title":"The future - Using Buildx to make Multi-arch a Way of Life"},{"location":"4-Multiarch-buildx-addendum/#buildx-seamless-multi-arch-builds-are-in-your-future","text":"Buildx has become a part of stable Docker builds with Docker CE 19.03 as an experimental feature. Since we have already enabled experimental features (if our docker version is 19.03 or greater) [if you don't remember run docker version ], we are ready to use buildx. We have already built our images above using a script. Let's see what the future holds...","title":"Buildx: Seamless multi-arch builds are in your future"},{"location":"4-Multiarch-buildx-addendum/#setup-buildx","text":"First, we will create a new builder: docker buildx create --name multi-arch Next, we will set this to our active builder: docker buildx use multi-arch Then, we will bootstrap it. At this point it will look for the supported architectures on our system using qemu and list them for us. docker buildx inspect --bootstrap Finally, we can see our current builders with: docker buildx ls Here is how this all looks from my mac: Notice, that the 2 architectures we want to use (s390x and amd64) are both supported on my system. This is because the qemu emulation is set up for s390x, arm (multiple versions), and ppc64le and my host platform is amd64 (in this case). This will work on any machine once its set up: either out-of-the-box with docker desktop for mac or windows or after you enable qemu on your linux box like we did above with the gmoney23/binfmt image.","title":"Setup Buildx"},{"location":"4-Multiarch-buildx-addendum/#login-to-our-docker-repo","text":"Set DOCKER_REPO=<docker_username> which for me is: DOCKER_REPO=gmoney23 Set yours accordingly, then do a docker login: docker login -u ${DOCKER_REPO}","title":"Login to our Docker Repo"},{"location":"4-Multiarch-buildx-addendum/#use-buildx-to-create-multi-arch-images","text":"Now, we can create a multi-architecture image for the three (x, z, and power) architectures (and more if desired) in one command: docker buildx build --platform linux/amd64,linux/s390x,linux/ppc64le -t ${DOCKER_REPO}/buildx-hello-node:1.0 \"${MULTIARCH_HOME}\"/node-web-app --push Info We are using the DOCKER_REPO we just logged in to with the environment variable we just set above* We can look at our newly pushed image to verify with: docker run mplatform/mquery ${DOCKER_REPO}/buildx-hello-node:1.0 To see the full manifest list we can inspect it with: docker manifest inspect ${DOCKER_REPO}/buildx-hello-node:1.0 Buildx isn't just limited to multi-arch builds with statics, it can also register remote builders to build images on those machines. To do this you will want to define multiple docker contexts which you can use to define multiple machines as detailed here . Then you can target these with your buildx builders. Again, if you want to check it out see the buildx github . Also, if you want to see a walk-through on buildx with arm see Building Multi-Arch Images for Arm and x86 with Docker Desktop . Additionally, in 2020 a wild walk-through on buildx appeared . It is worth checking out if you need a little more detail. We can use buildx to simplify the multi-arch process even further by bringing our build pipeline together in one place and allowing us to concurrently construct images for all arches that we can use in our manifest lists to \"create\" multi-arch images in one command. Now, it's time to use the multi-arch images in Kubernetes!","title":"Use Buildx to create multi-arch images"},{"location":"4-Multiarch-buildx-addendum/#optional-section","text":"","title":"OPTIONAL SECTION"},{"location":"4-Multiarch-buildx-addendum/#extra-content-most-users-should-skip-the-past-the-manual-way-aka-herding-cats","text":"Note This optional path is a manual collection of tasks to build images in more depth if you want more detail. This is purely for educational purposes if something in the script didn't make sense or you want further material and not part of the main path.","title":"Extra Content: [Most Users Should Skip]: The past - The Manual Way aka Herding Cats"},{"location":"4-Multiarch-buildx-addendum/#summary","text":"The knowledge that you can build and push multiarch images with one command is splendid, it fills you with determination","title":"Summary"},{"location":"4-Multiarch-buildx-addendum/#part-5-kubernetes-time","text":"","title":"Part 5: Kubernetes Time"},{"location":"4-Multiarch-manual-addendum/","text":"The past - The Manual Way aka Herding Cats \u00b6 If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document. Overview of Process the Manual Way - Building on Separate Machines \u00b6 Info Below is a manual collection of tasks to build images in more depth if you want more detail. This is purely for educational purposes if something in the script didn't make sense or you want further material and not part of the main path to follow the main path click to go to part 5 ** 1. Build image for all architectures and push to docker registry. \u00b6 First is the list of basic steps, then a pattern you can use with both the mac and command prompt commands. Basic Steps \u00b6 a. Build and push image for s390x (Go onto an s390x linux instance, ssh in) i . Get your code onto the instance ( i . e . download git files onto the machine or git clone https : // github . com / siler23 / MultiArchMultiArchDockerKubernetes . git ) ii . Get to the directory with the code ( cd MultiArchDockerKubernetes ) iii . Build the image using docker ( docker build - t myrepo / outyet - s390x .) PROXY : add build args for PROXY using previously set variables or if not reset variables and then run ( docker build - t myrepo / outyet - x86 --build-arg http_proxy=$http_proxy --build-arg https_proxy=$https_proxy --build-arg no_proxy=\"$no_proxy\" .) iv . Docker login to your docker registry ( docker login registry_address ) [ only need to do once for each command prompt / terminal session on a machine ] v . Docker push image ( docker push myrepo / outyet - s390x ) b. Build and push image for x86 (Go onto an x86 computer with docker, probably your workstation) i . Get your code onto the instance if not already there ( i . e . git clone https : // github . com / siler23 / MultiArchDockerKubernetes . git ) ii . Get to the directory with the code and Dockerfile ( cd MultiArchDockerKubernetes ) iii . Build the image using docker ( docker build - t myrepo / outyet - x86 .) PROXY : add build args for PROXY using previously set variables or if not reset variables and then run ( docker build - t myrepo / outyet - x86 --build-arg http_proxy=%http_proxy% --build-arg https_proxy=%https_proxy% --build-arg no_proxy=\"%no_proxy%\" .) iv . Docker login to your docker registry ( docker login registry_address ) v . Docker push image ( docker push myrepo / outyet - x86 ) 2. Make manifest list \u00b6 Tip You only need to do this from one computer which for ease of access should probably be your workstation computer rather than a server. Make a manifest list (a multi-architecture image reference) out of that individual images your pushed. Basically, you are mapping both images to the same tag so that when someone asks for the image via docker pull gmoney23/outyet it will have a pointer to the correct image for your architecture automatically (i.e. if you are on s390x it will point to layers from gmoney23/outyet-s390x or if you are on x86 it will point to layers from gmoney23/outyet-x86) while using the same label (i.e. gmoney23/outyet). This makes it so users of the image don\u2019t have to worry about using different tags for different architectures so the Dockerfiles can be the same for different applications across architectures (i.e. the same Dockerfile can be used for a node application on z and x86 or a go application on z and x86). Docker Manifest: An experimental feature \u00b6 Recommended Option Unless need added features, use this built-in command and skip the optional section. If need added features, do the OPTIONAL section marked OPTIONAL below. Tip You only need to do this from one computer which for ease of access should probably be your workstation computer rather than a server. Login to your Docker Repo [Account] \u00b6 Set DOCKER_REPO=<docker_username> which for me is: DOCKER_REPO=gmoney23 Set yours accordingly, then do a docker login: docker login -u ${DOCKER_REPO} Enter your password when prompted: IF USING PROXY Make sure your http_proxy, https_proxy, and no_proxy our set ***if pushing to a repo outside of your internal network. Push versioned first \u00b6 Tip Replace gmoney23 with your registry in the commands below docker manifest create gmoney23/outyet:1.0 gmoney23/outyet-s390x:1.0 gmoney23/outyet-x86:1.0 docker manifest push -p gmoney23/outyet:1.0 (It takes a minute to push, please be patient) If you want to inspect your manifest, use: docker manifest inspect gmoney23/outyet:1.0 Then push latest for current versioned \u00b6 Tip Replace gmoney23 with your registry in the commands below docker manifest create gmoney23/outyet gmoney23/outyet-s390x gmoney23/outyet-x86 docker manifest push -p gmoney23/outyet (It takes a minute to push, please be patient) -p is important so you can push new manifests later for latest tag (no version tag defaults to latest as in this example) If you want to inspect your manifest, use: docker manifest inspect gmoney23/outyet But what if I already pushed a latest before and want to update it with my new latest version? For example, you just pushed version 2.0 of your app, you can update the existing manifest by using: docker manifest create --amend gmoney23/outyet gmoney23/outyet-s390x gmoney23/outyet-x86 This will replace the new latest manifest with the old one after you have already updated the latest individual images to the version 2.0 images. You can also make a pattern for all of this if you want to automate the process or just type it out for each image... Time to go to the last stage, unless you need that pesky manifest-tool. Part 5: Now, it's time to get these images into Kubernetes \u00b6 OPTIONAL [Most Users Should Skip]: Install Manifest tool if need added features \u00b6 The docker manifest command is experimental because while it does create manifests and push it doesn't have other features yet such as pushing manifest from files. If you want these extra features you can install the manifest tool here. If not, I would suggest just using the docker manifest command as its generally integrated better with docker. For example, once you docker login you don't need to enter username/password like you do with manifest-tool. Tip You only need to do this from one computer which for ease of access should probably be your workstation computer rather than a server. If golang not yet installed, install here with package for your os/arch here If git not yet installed, see instructions here for how to install for your os Git install instructions If using proxy , set git proxy settings. An example would be if you want to use proxy for all git push/pull set: git config --global http.proxy http://proxyUsername:proxyPassword@proxy.server.com:port replacing with your proxy values. For more different git proxy configurations for your specific needs so this gist on using git with proxies Install manifest-tool here with package for your os/arch here After all s390x and amd64 images are pushed (Only do the do following if you haven't done with the docker manifest command) \u00b6 Tip You only need to do this from one computer which for ease of access should probably be your workstation computer rather than a server. Push versioned manifest first \u00b6 First, you will update the yaml files in the given directory of the image you are trying to push (in this case outyet) as well as change into the directory. Then, switch my username/password in the following command for yours for the image repository you are pushing to and: IF USING PROXY: make sure your http_proxy , https_proxy , and no_proxy are set if pushing to a repo outside of your internal network. manifest-tool --username gmoney23 --password *** push from-spec smallest-outyet/vmanifest.yaml Then push latest manifest for current versioned \u00b6 manifest-tool --username gmoney23 --password *** push from-spec smallest-outyet/manifest.yaml Having your hands dirty fills you with determination . Part 5: Kubernetes Time \u00b6","title":"The past - The Manual Way aka Herding Cats"},{"location":"4-Multiarch-manual-addendum/#the-past-the-manual-way-aka-herding-cats","text":"If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document.","title":"The past - The Manual Way aka Herding Cats"},{"location":"4-Multiarch-manual-addendum/#overview-of-process-the-manual-way-building-on-separate-machines","text":"Info Below is a manual collection of tasks to build images in more depth if you want more detail. This is purely for educational purposes if something in the script didn't make sense or you want further material and not part of the main path to follow the main path click to go to part 5 **","title":"Overview of Process the Manual Way - Building on Separate Machines"},{"location":"4-Multiarch-manual-addendum/#1-build-image-for-all-architectures-and-push-to-docker-registry","text":"First is the list of basic steps, then a pattern you can use with both the mac and command prompt commands.","title":"1. Build image for all architectures and push to docker registry."},{"location":"4-Multiarch-manual-addendum/#basic-steps","text":"a. Build and push image for s390x (Go onto an s390x linux instance, ssh in) i . Get your code onto the instance ( i . e . download git files onto the machine or git clone https : // github . com / siler23 / MultiArchMultiArchDockerKubernetes . git ) ii . Get to the directory with the code ( cd MultiArchDockerKubernetes ) iii . Build the image using docker ( docker build - t myrepo / outyet - s390x .) PROXY : add build args for PROXY using previously set variables or if not reset variables and then run ( docker build - t myrepo / outyet - x86 --build-arg http_proxy=$http_proxy --build-arg https_proxy=$https_proxy --build-arg no_proxy=\"$no_proxy\" .) iv . Docker login to your docker registry ( docker login registry_address ) [ only need to do once for each command prompt / terminal session on a machine ] v . Docker push image ( docker push myrepo / outyet - s390x ) b. Build and push image for x86 (Go onto an x86 computer with docker, probably your workstation) i . Get your code onto the instance if not already there ( i . e . git clone https : // github . com / siler23 / MultiArchDockerKubernetes . git ) ii . Get to the directory with the code and Dockerfile ( cd MultiArchDockerKubernetes ) iii . Build the image using docker ( docker build - t myrepo / outyet - x86 .) PROXY : add build args for PROXY using previously set variables or if not reset variables and then run ( docker build - t myrepo / outyet - x86 --build-arg http_proxy=%http_proxy% --build-arg https_proxy=%https_proxy% --build-arg no_proxy=\"%no_proxy%\" .) iv . Docker login to your docker registry ( docker login registry_address ) v . Docker push image ( docker push myrepo / outyet - x86 )","title":"Basic Steps"},{"location":"4-Multiarch-manual-addendum/#2-make-manifest-list","text":"Tip You only need to do this from one computer which for ease of access should probably be your workstation computer rather than a server. Make a manifest list (a multi-architecture image reference) out of that individual images your pushed. Basically, you are mapping both images to the same tag so that when someone asks for the image via docker pull gmoney23/outyet it will have a pointer to the correct image for your architecture automatically (i.e. if you are on s390x it will point to layers from gmoney23/outyet-s390x or if you are on x86 it will point to layers from gmoney23/outyet-x86) while using the same label (i.e. gmoney23/outyet). This makes it so users of the image don\u2019t have to worry about using different tags for different architectures so the Dockerfiles can be the same for different applications across architectures (i.e. the same Dockerfile can be used for a node application on z and x86 or a go application on z and x86).","title":"2. Make manifest list"},{"location":"4-Multiarch-manual-addendum/#docker-manifest-an-experimental-feature","text":"Recommended Option Unless need added features, use this built-in command and skip the optional section. If need added features, do the OPTIONAL section marked OPTIONAL below. Tip You only need to do this from one computer which for ease of access should probably be your workstation computer rather than a server.","title":"Docker Manifest: An experimental feature"},{"location":"4-Multiarch-manual-addendum/#login-to-your-docker-repo-account","text":"Set DOCKER_REPO=<docker_username> which for me is: DOCKER_REPO=gmoney23 Set yours accordingly, then do a docker login: docker login -u ${DOCKER_REPO} Enter your password when prompted: IF USING PROXY Make sure your http_proxy, https_proxy, and no_proxy our set ***if pushing to a repo outside of your internal network.","title":"Login to your Docker Repo [Account]"},{"location":"4-Multiarch-manual-addendum/#push-versioned-first","text":"Tip Replace gmoney23 with your registry in the commands below docker manifest create gmoney23/outyet:1.0 gmoney23/outyet-s390x:1.0 gmoney23/outyet-x86:1.0 docker manifest push -p gmoney23/outyet:1.0 (It takes a minute to push, please be patient) If you want to inspect your manifest, use: docker manifest inspect gmoney23/outyet:1.0","title":"Push versioned first"},{"location":"4-Multiarch-manual-addendum/#then-push-latest-for-current-versioned","text":"Tip Replace gmoney23 with your registry in the commands below docker manifest create gmoney23/outyet gmoney23/outyet-s390x gmoney23/outyet-x86 docker manifest push -p gmoney23/outyet (It takes a minute to push, please be patient) -p is important so you can push new manifests later for latest tag (no version tag defaults to latest as in this example) If you want to inspect your manifest, use: docker manifest inspect gmoney23/outyet But what if I already pushed a latest before and want to update it with my new latest version? For example, you just pushed version 2.0 of your app, you can update the existing manifest by using: docker manifest create --amend gmoney23/outyet gmoney23/outyet-s390x gmoney23/outyet-x86 This will replace the new latest manifest with the old one after you have already updated the latest individual images to the version 2.0 images. You can also make a pattern for all of this if you want to automate the process or just type it out for each image... Time to go to the last stage, unless you need that pesky manifest-tool.","title":"Then push latest for current versioned"},{"location":"4-Multiarch-manual-addendum/#part-5-now-its-time-to-get-these-images-into-kubernetes","text":"","title":"Part 5: Now, it's time to get these images into Kubernetes"},{"location":"4-Multiarch-manual-addendum/#optional-most-users-should-skip-install-manifest-tool-if-need-added-features","text":"The docker manifest command is experimental because while it does create manifests and push it doesn't have other features yet such as pushing manifest from files. If you want these extra features you can install the manifest tool here. If not, I would suggest just using the docker manifest command as its generally integrated better with docker. For example, once you docker login you don't need to enter username/password like you do with manifest-tool. Tip You only need to do this from one computer which for ease of access should probably be your workstation computer rather than a server. If golang not yet installed, install here with package for your os/arch here If git not yet installed, see instructions here for how to install for your os Git install instructions If using proxy , set git proxy settings. An example would be if you want to use proxy for all git push/pull set: git config --global http.proxy http://proxyUsername:proxyPassword@proxy.server.com:port replacing with your proxy values. For more different git proxy configurations for your specific needs so this gist on using git with proxies Install manifest-tool here with package for your os/arch here","title":"OPTIONAL [Most Users Should Skip]: Install Manifest tool if need added features"},{"location":"4-Multiarch-manual-addendum/#after-all-s390x-and-amd64-images-are-pushed-only-do-the-do-following-if-you-havent-done-with-the-docker-manifest-command","text":"Tip You only need to do this from one computer which for ease of access should probably be your workstation computer rather than a server.","title":"After all s390x and amd64 images are pushed (Only do the do following if you haven't done with the docker manifest command)"},{"location":"4-Multiarch-manual-addendum/#push-versioned-manifest-first","text":"First, you will update the yaml files in the given directory of the image you are trying to push (in this case outyet) as well as change into the directory. Then, switch my username/password in the following command for yours for the image repository you are pushing to and: IF USING PROXY: make sure your http_proxy , https_proxy , and no_proxy are set if pushing to a repo outside of your internal network. manifest-tool --username gmoney23 --password *** push from-spec smallest-outyet/vmanifest.yaml","title":"Push versioned manifest first"},{"location":"4-Multiarch-manual-addendum/#then-push-latest-manifest-for-current-versioned","text":"manifest-tool --username gmoney23 --password *** push from-spec smallest-outyet/manifest.yaml Having your hands dirty fills you with determination .","title":"Then push latest manifest for current versioned"},{"location":"4-Multiarch-manual-addendum/#part-5-kubernetes-time","text":"","title":"Part 5: Kubernetes Time"},{"location":"5-Deploy-to-Kubernetes/","text":"5. Kubernetes Time \u00b6 We will use the multi-arch docker images we have created to make deployments that will run across s390x and amd64 nodes as well as force pods to go to either s390x or amd64 nodes. We will cover the use of deployments, services, configmaps, jobs, and cronjobs including how to easily declare a proxy to your application. If you are starting to lose your determination \u00b6 The inspiring picture fills you with determination . Don't forget to set the multi-arch lab home directory \u00b6 export MULTIARCH_HOME= full path to directory of multiarch repo If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document. If you don't have a Kubernetes Environment you can use Docker \u00b6 To set up Kubernetes on Docker for Mac follow these Mac instructions To set up Kubernetes on Docker for Windows follow these Windows instructions Info If you follow this path you will have a single architecture cluster. Thus, when you try to force things to run on a different architecture node, you should expect to get a pending pod. This demonstrates that the multiarch capability is working and checking to ensure image and node architectures match. If you run on a multiarch cluster, you can tell things are working in an \"arguably cooler\" way (pods move to a node of the specified architecture). Nevertheless, both methods demonstrate the multiarch selection capabilities. Moreover, Docker is seemingly ubiquitous, so I have included pointers to the docs above to set up Kubernetes with Docker just in case you don't have access to a multiarch cluster at the moment. Running on windows [Mac/Linux Users Skip This] The commands listed are bash commands. In order to use bash on Windows 10 see Enabling Bash on PC . If you do that, all of the commands below will work in a bash script for you. If you don't want to do that you can also use the command prompt. Just replace the $var with %var% and var=x with set var=x . The kubectl commands themselves are the same across operating systems. An example is changing the bash command: peer_pod=\"$(kubectl get pods -l app=node-web,lab=multi-arch-docker -o jsonpath='{.items[*].metadata.name}')\" to the Windows command: kubectl get pods -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' set peer_pod = result where result is the output of the previous command and when referencing it change bash: kubectl exec $peer_pod -- ash -c \"uname -m\" (bash) to Windows: kubectl exec %peer_pod% -- ash -c \"uname -m\" (Windows) Again, you'll notice the kubectl command is exactly the same. We are just changing the variable use reference in bash to command prompt style. Also that $() was a bash way to run the command its in own shell and feed the output to our command so I broke the command into two for windows instead. Let's create our first Deployment using our node-web-app \u00b6 Create the deployment with: kubectl apply -f \" ${ MULTIARCH_HOME } \" /node-web-app/deployment.yaml You can think of the deployment as deploying all of the parts needed to manage your running application. The deployment itself manages updates and configuration of your application. It creates a ReplicaSet that manages the number of replicas [pods] that are up at a time for a given application with a control loop. A pod is the smallest unit in Kubernetes and is made up of all the containers in a given deployment (application) that have to be scheduled on the same node. Now, let's use a label to determine one pod that belongs to your deployment. Then: peer_pod = \" $( kubectl get pods -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' ) \" A note about jsonpath \u00b6 You may have noticed the -o jsonpath='{.items[*].metadata.name}') in the previous example. Kubernetes has jsonpath capability built-in. Jsonpath is a query language that lets us extract the bits of a json document, in this case the bits of the Kubernetes json document (found with kubectl get pods $peer_pod -o json ), to get the information we need. This enables us to easily find up to date information about the objects in our cluster and allows us to use this information to make automation scripts among other things. In this example, we are finding the name of our pod in an automated way. A note about labels \u00b6 We used a selector to select a pod with a specific set of labels. We can also do this with other objects that have labels such as deployments with: kubectl get deploy -l app = node-web,lab = multi-arch-docker Labels are key-value pairs attached to objects to specify certain qualities and enable users and controllers to select objects based on these qualities. For example, we can attach a label to a pod such as lab and look for pods that have that label to see all pods that are part of a lab. Furthermore, we can look at the value of the label and see all pods labeled for a specific lab such as multi-arch-docker . The replicaSets and deployments introduced above use labels to keep track of which objects they are controlling and services (which we'll dive into later) use selectors of certain labels to see to which pods they should route traffic. In order to see all the labels on a specific object we can use the --show-labels option such as: kubectl get pod ${ peer_pod } --show-labels Changing Architectures \u00b6 Since the node-web-app has a base image with ash, we can run ash on this pod to find out the architecture of the node it's running on: kubectl exec $peer_pod -- ash -c \"uname -m\" Depending on if it's currently running on x86 or s390x, we can force it to run on the other with a nodeSelector . (We could also force it to stay on that arch, but that's no fun). A nodeSelector is a special label which needs to be satisfied for a pod to be assigned to a node. The nodeSelector beta.kubernetes.io/arch: is for deploying pods on a node with the specified architecture. (i.e. specify z with beta.kubernetes.io/arch: s390x ) If it's running on x86 now, force z \u00b6 deployment = $( kubectl get deploy -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' ) kubectl patch deployment $deployment --patch \" $( cat \" ${ MULTIARCH_HOME } \" /zNodeSelector.yaml ) \" The file itself just includes the necessary nodeSelector with the correct number of {} for json placement Now, a new pod will be created on the target architecture. Thus, first we will get our pod name and then check it's architecture again after waiting ten seconds first with sleep to give the new pod time to be created. sleep 10 && peer_pod = \" $( kubectl get pods -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[0].metadata.name}' ) \" confirm the architecture changed with: kubectl exec $peer_pod -- ash -c \"uname -m\" Warning If you are using Docker, you will get an error here due to the inability to reschedule the pod to a node of a different architecture. If it's running on IBM Z, force x86 \u00b6 deployment = $( kubectl get deploy -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' ) kubectl patch deployment $deployment --patch \" $( cat \" ${ MULTIARCH_HOME } \" /xNodeSelector.yaml ) \" The file itself just includes the necessary nodeSelector with the correct number of {} for json placement Now, a new pod will be created on the target architecture. Thus, first we will get our pod name and then check it's architecture again after waiting ten seconds first with sleep to give the new pod time to be created. sleep 10 && peer_pod = \" $( kubectl get pods -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[0].metadata.name}' ) \" confirm the architecture changed with: kubectl exec $peer_pod -- ash -c \"uname -m\" Warning If you are using Docker, you will get an error here due to the inability to reschedule the pod to a node of a different architecture*** Overview of NodeSelector (What did we just do?) \u00b6 The nodeSelector element can be very useful when working with multiple architectures because in certain cases you only want your workload to go on one architecture of your multi-architecture cluster. For example, you may want a database to run without the need for sharding on LinuxONE and a deep learning workload to run on x86. This is one of the ways you can tell your cluster to do that. To see all the ways to assign pods to nodes look at Assigning Pods to Nodes Viewing our Nodejs Web application in Kubernetes \u00b6 Create a Service to talk to the Outside World \u00b6 A service exposes our application to traffic from both inside the cluster and to the outside world depending on the service type . In order to create the service for our node-web-app we will apply the yaml files as before using: kubectl apply -f \" ${ MULTIARCH_HOME } \" /node-web-app/service.yaml service/multi-arch-docker-node-web created Then, we can get the address our service exposed by first finding our service using label selectors again with the command: service = $( kubectl get svc -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[0].metadata.name}' ) Now we can find the external address the service exposed for us. Admin can run this command or check the console for the Proxy IP \u00b6 [ -z \" $CLUSTERIP \" ] && CLUSTERIP = $( kubectl get nodes -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) Non-admin Needs to get the IP from Admin \u00b6 CLUSTERIP=<your_proxy_node_ip> Visit Node Web App \u00b6 NODEPORT=$(kubectl get svc $service -o jsonpath='{.spec.ports[0].nodePort}') Now, visit the app at the url you get from the following command: echo \"http:// ${ CLUSTERIP } : ${ NODEPORT } \" in our browser A NodePort opens up all externally accessible nodes in the cluster at a fixed port. This is a little messy in practice, but good for demos, etc. because of its ease. After visiting our app, we can delete it and our service with: kubectl delete -f \" ${ MULTIARCH_HOME } \" /node-web-app/deployment.yaml deployment.extensions \"multi-arch-docker-node-web\" deleted kubectl delete -f \" ${ MULTIARCH_HOME } \" /node-web-app/service.yaml service \"multi-arch-docker-node-web\" deleted Let's create our first Go Deployment \u00b6 We'll use the example-go-server image and some quick kubectl one-line commands to spin things up. The advantage of these is that it is easy to get things running. However, your configuration isn't automatically saved and configured, which is why managing with .yaml files in production is preferred. Make a Quick Deployment and force on z \u00b6 kubectl run go-example --image = gmoney23/example-go-server --port 5000 --image-pull-policy = Always && kubectl patch deployment go-example --patch \" $( cat \" ${ MULTIARCH_HOME } \" /zNodeSelector.yaml ) \" kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead. deployment.apps/go-example created kubectl get deploy go-example Warning If you are using Docker on a non-z architecture, you will get a pending pod here along with the original running pod due to the inability to reschedule the pod to a node with the z (s390x) architecture. You can proceed as the original pod is still up and running so the workload faces no downtime. Expose the Deployment to the Outside World \u00b6 This deployment is available. However, I can't access it from nodes outside of my cluster which is no good for a web app, so lets expose it to external connections by adding a nodePort service kubectl expose deployment go-example --type = NodePort kubectl get svc go-example Now if I go to my $CLUSTERIP:32532 as specified in port I can see my app running. To print out this as a url to go to like before run the following commands: NODEPORT=$(kubectl get svc go-example -o jsonpath='{.spec.ports[0].nodePort}') echo \"http://${CLUSTERIP}:${NODEPORT}\" Note Notice that this parsed the NodePort that was visible in the earlier output using jsonpath in order to easily print it for us with echo like before. Now I can save the deployment and service I created to a file with --export. kubectl get deployment go-example -o yaml --export > go-example-deployment.yaml kubectl get svc go-example -o yaml --export > go-example-svc.yaml Find the Node Hosting my Pod \u00b6 Since this pod doesn't have a bash shell, since it was made from scratch, I'll use kubectl get pods -o wide to figure out which node it is running on. (We should know which architecture each host node is or could get that from our cluster admin.) kubectl get pods -l run = go-example -o wide Clean Up Sample Go Deployment \u00b6 We can delete the deployment and service by using kubectl delete resource name such as kubectl delete deployment deployment_name . For our sample go deployment this means: kubectl delete deployment go-example kubectl delete service go-example Deal with Proxies with a ConfigMap \u00b6 In our first 2 deployments, our apps were confined within our own premises, not requiring any calls to the outside world. However, in many cases, an app will need to make calls to the outside world for things like live updates and information from web sites. Soon, our outyet deployment will need to check the golang git page to see if version 1.11 of go has been released. Later, our href application will need to call out to websites to check their internal and external hrefs. Many applications require this functionality and many corporations have proxies that prevent apps running in containers from making these calls natively. In order to achieve this functionality, we will be using ConfigMaps. A configmap enables us to specify our configuration values in one place, where they can be referenced by an application. This way if a user needs different values for something such as a proxy, they can be changed in one place instead of digging into the application. Here is our proxy ConfigMap: Regular Users If a user doesn't need a proxy they can just not apply this ConfigMap since we will make it optional in the deployment. Proxy users Only if you use a proxy Please put your proxy values here and apply this ConfigMap with: kubectl apply -f \" ${ MULTIARCH_HOME } \" /proxy-configmap.yaml Explore how to create deployments and services from yaml \u00b6 Here we will explore a sample deployment file to use in our cluster. Time to Deploy YAML \u00b6 This deployment attaches a label of small-outyet to identify it and selects pods with that same label running the gmoney23/smallest-outyet:1.0 image. The port (as we learned before with docker) is 8080 for the outyet family of applications. Additionally, the deployment references the configMap we previously created to deal with proxies. The envFrom means that all of the variables from the configMap will be deployed. The configMapRef marked as optional: true means that if you don't have a proxy and thus didn't apply the referenced configMap, you are not affected at all. This allows us to make an application that works for both proxy and regular users. Finally, the imagePullPolicy marked as Always means that we will check for a new version of the image every time we run a container from it. The default is that this happens for an image marked latest/no tag because that image is frequently updated with new versions. The default for images tagged with a version number is to only pull the image IfNotPresent which means if it is not on the machine. This is because usually a versioned release is updated with a new version rather than updating that same versioned image. If that's not the case, we can specify a different imagePullPolicy for those images like we do here with Always which overrides the default. If your image lies in a private repository, we can also add a secret to reference it without having to docker login to the repository first. See Pulling an image from a private registry . This can be important in Kubernetes, when many times the user should not have direct access to the host server and you want to be able to pull from a private registry with authentication. That was a mouthful ... Now, we can simply apply this file to create the deployment. Time to Service YAML \u00b6 kubectl apply -f \" ${ MULTIARCH_HOME } \" /smallest-outyet/deployment.yaml deployment.extensions/multi-arch-docker-smallest-outyet created Now, to enable us to connect to this app, we need to deploy a service. We will create a yaml file for this as well called service.yaml . This service again uses the NodePort type mapping port 8080 as the container port and internal port to connect to the external NodePort . We can apply this to create the service. kubectl apply -f \" ${ MULTIARCH_HOME } \" /smallest-outyet/service.yaml service/multi-arch-docker-smallest-outyet created We can look at how the service maps to the pod by looking at the endpoints of the service. kubectl get ep -l app = smallest-outyet,lab = multi-arch-docker Now, I can scale the deployment to having 2 replicas instead of 1. deployment=multi-arch-docker-smallest-outyet kubectl patch deployment $deployment -p '{\"spec\": {\"replicas\": 2}}' Let's look at the endpoints for the service again. kubectl get ep -l app = smallest-outyet,lab = multi-arch-docker We can see the two pods' endpoints using the service instead of the one before. Get the Access Info for Outyet \u00b6 To access the application itself, I can get the IP and NodePort using kubectl Admin Can run this command or check the console for the Proxy IP [ -z \" $CLUSTERIP \" ] && CLUSTERIP = $( kubectl get nodes -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) Non-admin Needs to get the IP from Admin CLUSTERIP = <your_proxy_node_ip> Time to See if it's Outyet \u00b6 NODEPORT=$(kubectl get svc -l app=smallest-outyet,lab=multi-arch-docker -o jsonpath='{.items[0].spec.ports[0].nodePort}') echo \"http:// ${ CLUSTERIP } : ${ NODEPORT } \" I can plug this address into my browser to view the app. Clean Up \u00b6 To clean app I can delete the deployment using the yaml I created it with. The same goes for the service. kubectl delete -f \" ${ MULTIARCH_HOME } \" /smallest-outyet/deployment.yaml deployment.extensions \"multi-arch-docker-smallest-outyet\" deleted kubectl delete -f \" ${ MULTIARCH_HOME } \" /smallest-outyet/service.yaml service \"multi-arch-docker-smallest-outyet\" deleted New Web App, Similar Steps for Access: \u00b6 Small-Outyet Deserves a Chance to Run \u00b6 kubectl apply -f \" ${ MULTIARCH_HOME } \" /small-outyet/deployment.yaml deployment.extensions/multi-arch-docker-small-outyet created kubectl apply -f \" ${ MULTIARCH_HOME } \" /small-outyet/service.yaml service/multi-arch-docker-small-outyet created service = multi-arch-docker-small-outyet peer_pod = \" $( kubectl get pods -l app = small-outyet,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' ) \" Admin can run this command or to find out the Proxy IP \u00b6 [ -z \" $CLUSTERIP \" ] && CLUSTERIP = $( kubectl get nodes -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) Non-admin Needs to get the IP from the Admin \u00b6 CLUSTERIP=<your_proxy_node_ip> It's still out! \u00b6 NODEPORT = $( kubectl get svc $service -o jsonpath = '{.spec.ports[0].nodePort}' ) echo \"http:// ${ CLUSTERIP } : ${ NODEPORT } \" We can visit this address, to visit our small-outyet app. We can follow similar steps for all of the outyets and example-go-server since they are all web applications with similar constraints. Clean up Small-Outyet \u00b6 kubectl delete -f \" ${ MULTIARCH_HOME } \" /small-outyet/deployment.yaml deployment.extensions \"multi-arch-docker-small-outyet\" deleted kubectl delete -f \" ${ MULTIARCH_HOME } \" /small-outyet/service.yaml service \"multi-arch-docker-small-outyet\" deleted Using Jobs with a CronJob \u00b6 A job runs a number of pods in order to achieve a given number of successful completions at which point it is done. A CronJob runs a job on a time-based schedule. In this example, we will use a cronjob to launch our trusty href-counter , every minute. We will then change the environment values sent into href-counter, to get it to switch to a different website and look at its logs to show us the results. Instead of mounting the environment variables directly in the pod we will be using a new configmap and the one for proxies we created before to map configuration values to a container instead of hard coding them in (if applicable) First, lets create our ConfigMap \u00b6 Our configmap.yaml is as follows: It simply maps the http-url for us to hit with the href-tool to the site-url configmap with key http-url. We will be patching this when we want to update our container's environment value. Let's create our configmap: kubectl apply -f \" ${ MULTIARCH_HOME } \" /href-counter/configmap.yaml configmap/multi-arch-docker-site-url created Next, we have to make our CronJob. The following yaml will suffice: This CronJob schedules a job every minute using href-counter as our label and our image as gmoney23/href:1.0 . We also use our new ConfigMap in the valueFrom field of the env field where we specify our configMapKeyRef to reference a specific key. Finally, we connect to our proxy ConfigMap again since this app makes calls to outside websites. We should be all set. Time to make the cronjob. kubectl apply -f \" ${ MULTIARCH_HOME } \" /href-counter/cronjob.yaml cronjob.batch/multi-arch-docker-href-counter created After waiting a few minutes , run the logs to see the results: kubectl logs -l app = href-counter,lab = multi-arch-docker {\"internal\":5,\"external\":89} {\"internal\":5,\"external\":89} {\"internal\":5,\"external\":86} Finally, it's time to patch the configmap and see the changes... kubectl patch configmap multi-arch-docker-site-url -p '{\"data\": {\"http-url\": \"http://google.com\"}}' With the configmap patched we should be able to see the changes to the logs of the pods run by the jobs from the CronJob though you might have to wait up to a minute to see a change ( Note: usually only the 3 most recent pods at a given time are available for log reading ): kubectl logs -l app = href-counter,lab = multi-arch-docker {\"internal\":5,\"external\":89} {\"internal\":5,\"external\":86} {\"internal\":8,\"external\":11} Indeed, our values have changed. Our work is complete, time to clean up. kubectl delete -f \" ${ MULTIARCH_HOME } \" /href-counter/cronjob.yaml cronjob.batch \"multi-arch-docker-href-counter\" deleted kubectl delete -f \" ${ MULTIARCH_HOME } \" /href-counter/configmap.yaml configmap \"multi-arch-docker-site-url\" deleted In reality, our CronJob could be used to do anything from running the logs at the end of a day, to sending emails to a list of participants weekly, to running health checks on an application every minute to other automated tasks that need to be done every period of time. If you need more Kubernetes skills, cover your bases with Kubernetes basics . Additionally, if you have a hard time figuring out which api to use for a given type look no further Closing Note \u00b6 THAT'S ALL FOLKS! Note to Self Oops that was off message. I meant to say: Knowing you have a guide as a template for future success fills you with determination . P.S. My ability to stay on message and reminisce about Undertale fills me with determination .* P.P.S What ever will I do next * Additional Topics \u00b6 After finishing everything here, check out: Packaging Applications and Services with Kubernetes Operators Red Hat OpenShift Free Interactive Online Learning HOME \u00b6","title":"5. Kubernetes Time"},{"location":"5-Deploy-to-Kubernetes/#5-kubernetes-time","text":"We will use the multi-arch docker images we have created to make deployments that will run across s390x and amd64 nodes as well as force pods to go to either s390x or amd64 nodes. We will cover the use of deployments, services, configmaps, jobs, and cronjobs including how to easily declare a proxy to your application.","title":"5. Kubernetes Time"},{"location":"5-Deploy-to-Kubernetes/#if-you-are-starting-to-lose-your-determination","text":"The inspiring picture fills you with determination .","title":"If you are starting to lose your determination"},{"location":"5-Deploy-to-Kubernetes/#dont-forget-to-set-the-multi-arch-lab-home-directory","text":"export MULTIARCH_HOME= full path to directory of multiarch repo If Using Proxy If using proxy , make sure you've read 0-ProxyPSA and have set your http_proxy , https_proxy , and no_proxy variables for your environment as specified there. Also note that for all docker run commands add the -e for each of the proxy environment variables as specified in that 0-ProxyPSA document.","title":"Don't forget to set the multi-arch lab home directory"},{"location":"5-Deploy-to-Kubernetes/#if-you-dont-have-a-kubernetes-environment-you-can-use-docker","text":"To set up Kubernetes on Docker for Mac follow these Mac instructions To set up Kubernetes on Docker for Windows follow these Windows instructions Info If you follow this path you will have a single architecture cluster. Thus, when you try to force things to run on a different architecture node, you should expect to get a pending pod. This demonstrates that the multiarch capability is working and checking to ensure image and node architectures match. If you run on a multiarch cluster, you can tell things are working in an \"arguably cooler\" way (pods move to a node of the specified architecture). Nevertheless, both methods demonstrate the multiarch selection capabilities. Moreover, Docker is seemingly ubiquitous, so I have included pointers to the docs above to set up Kubernetes with Docker just in case you don't have access to a multiarch cluster at the moment. Running on windows [Mac/Linux Users Skip This] The commands listed are bash commands. In order to use bash on Windows 10 see Enabling Bash on PC . If you do that, all of the commands below will work in a bash script for you. If you don't want to do that you can also use the command prompt. Just replace the $var with %var% and var=x with set var=x . The kubectl commands themselves are the same across operating systems. An example is changing the bash command: peer_pod=\"$(kubectl get pods -l app=node-web,lab=multi-arch-docker -o jsonpath='{.items[*].metadata.name}')\" to the Windows command: kubectl get pods -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' set peer_pod = result where result is the output of the previous command and when referencing it change bash: kubectl exec $peer_pod -- ash -c \"uname -m\" (bash) to Windows: kubectl exec %peer_pod% -- ash -c \"uname -m\" (Windows) Again, you'll notice the kubectl command is exactly the same. We are just changing the variable use reference in bash to command prompt style. Also that $() was a bash way to run the command its in own shell and feed the output to our command so I broke the command into two for windows instead.","title":"If you don't have a Kubernetes Environment you can use Docker"},{"location":"5-Deploy-to-Kubernetes/#lets-create-our-first-deployment-using-our-node-web-app","text":"Create the deployment with: kubectl apply -f \" ${ MULTIARCH_HOME } \" /node-web-app/deployment.yaml You can think of the deployment as deploying all of the parts needed to manage your running application. The deployment itself manages updates and configuration of your application. It creates a ReplicaSet that manages the number of replicas [pods] that are up at a time for a given application with a control loop. A pod is the smallest unit in Kubernetes and is made up of all the containers in a given deployment (application) that have to be scheduled on the same node. Now, let's use a label to determine one pod that belongs to your deployment. Then: peer_pod = \" $( kubectl get pods -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' ) \"","title":"Let's create our first Deployment using our node-web-app"},{"location":"5-Deploy-to-Kubernetes/#a-note-about-jsonpath","text":"You may have noticed the -o jsonpath='{.items[*].metadata.name}') in the previous example. Kubernetes has jsonpath capability built-in. Jsonpath is a query language that lets us extract the bits of a json document, in this case the bits of the Kubernetes json document (found with kubectl get pods $peer_pod -o json ), to get the information we need. This enables us to easily find up to date information about the objects in our cluster and allows us to use this information to make automation scripts among other things. In this example, we are finding the name of our pod in an automated way.","title":"A note about jsonpath"},{"location":"5-Deploy-to-Kubernetes/#a-note-about-labels","text":"We used a selector to select a pod with a specific set of labels. We can also do this with other objects that have labels such as deployments with: kubectl get deploy -l app = node-web,lab = multi-arch-docker Labels are key-value pairs attached to objects to specify certain qualities and enable users and controllers to select objects based on these qualities. For example, we can attach a label to a pod such as lab and look for pods that have that label to see all pods that are part of a lab. Furthermore, we can look at the value of the label and see all pods labeled for a specific lab such as multi-arch-docker . The replicaSets and deployments introduced above use labels to keep track of which objects they are controlling and services (which we'll dive into later) use selectors of certain labels to see to which pods they should route traffic. In order to see all the labels on a specific object we can use the --show-labels option such as: kubectl get pod ${ peer_pod } --show-labels","title":"A note about labels"},{"location":"5-Deploy-to-Kubernetes/#changing-architectures","text":"Since the node-web-app has a base image with ash, we can run ash on this pod to find out the architecture of the node it's running on: kubectl exec $peer_pod -- ash -c \"uname -m\" Depending on if it's currently running on x86 or s390x, we can force it to run on the other with a nodeSelector . (We could also force it to stay on that arch, but that's no fun). A nodeSelector is a special label which needs to be satisfied for a pod to be assigned to a node. The nodeSelector beta.kubernetes.io/arch: is for deploying pods on a node with the specified architecture. (i.e. specify z with beta.kubernetes.io/arch: s390x )","title":"Changing Architectures"},{"location":"5-Deploy-to-Kubernetes/#if-its-running-on-x86-now-force-z","text":"deployment = $( kubectl get deploy -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' ) kubectl patch deployment $deployment --patch \" $( cat \" ${ MULTIARCH_HOME } \" /zNodeSelector.yaml ) \" The file itself just includes the necessary nodeSelector with the correct number of {} for json placement Now, a new pod will be created on the target architecture. Thus, first we will get our pod name and then check it's architecture again after waiting ten seconds first with sleep to give the new pod time to be created. sleep 10 && peer_pod = \" $( kubectl get pods -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[0].metadata.name}' ) \" confirm the architecture changed with: kubectl exec $peer_pod -- ash -c \"uname -m\" Warning If you are using Docker, you will get an error here due to the inability to reschedule the pod to a node of a different architecture.","title":"If it's running on x86 now, force z"},{"location":"5-Deploy-to-Kubernetes/#if-its-running-on-ibm-z-force-x86","text":"deployment = $( kubectl get deploy -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' ) kubectl patch deployment $deployment --patch \" $( cat \" ${ MULTIARCH_HOME } \" /xNodeSelector.yaml ) \" The file itself just includes the necessary nodeSelector with the correct number of {} for json placement Now, a new pod will be created on the target architecture. Thus, first we will get our pod name and then check it's architecture again after waiting ten seconds first with sleep to give the new pod time to be created. sleep 10 && peer_pod = \" $( kubectl get pods -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[0].metadata.name}' ) \" confirm the architecture changed with: kubectl exec $peer_pod -- ash -c \"uname -m\" Warning If you are using Docker, you will get an error here due to the inability to reschedule the pod to a node of a different architecture***","title":"If it's running on IBM Z, force x86"},{"location":"5-Deploy-to-Kubernetes/#overview-of-nodeselector-what-did-we-just-do","text":"The nodeSelector element can be very useful when working with multiple architectures because in certain cases you only want your workload to go on one architecture of your multi-architecture cluster. For example, you may want a database to run without the need for sharding on LinuxONE and a deep learning workload to run on x86. This is one of the ways you can tell your cluster to do that. To see all the ways to assign pods to nodes look at Assigning Pods to Nodes","title":"Overview of NodeSelector (What did we just do?)"},{"location":"5-Deploy-to-Kubernetes/#viewing-our-nodejs-web-application-in-kubernetes","text":"","title":"Viewing our Nodejs Web application in Kubernetes"},{"location":"5-Deploy-to-Kubernetes/#create-a-service-to-talk-to-the-outside-world","text":"A service exposes our application to traffic from both inside the cluster and to the outside world depending on the service type . In order to create the service for our node-web-app we will apply the yaml files as before using: kubectl apply -f \" ${ MULTIARCH_HOME } \" /node-web-app/service.yaml service/multi-arch-docker-node-web created Then, we can get the address our service exposed by first finding our service using label selectors again with the command: service = $( kubectl get svc -l app = node-web,lab = multi-arch-docker -o jsonpath = '{.items[0].metadata.name}' ) Now we can find the external address the service exposed for us.","title":"Create a Service to talk to the Outside World"},{"location":"5-Deploy-to-Kubernetes/#admin-can-run-this-command-or-check-the-console-for-the-proxy-ip","text":"[ -z \" $CLUSTERIP \" ] && CLUSTERIP = $( kubectl get nodes -o jsonpath = \"{.items[0].status.addresses[0].address}\" )","title":"Admin can run this command or check the console for the Proxy IP"},{"location":"5-Deploy-to-Kubernetes/#non-admin-needs-to-get-the-ip-from-admin","text":"CLUSTERIP=<your_proxy_node_ip>","title":"Non-admin Needs to get the IP from Admin"},{"location":"5-Deploy-to-Kubernetes/#visit-node-web-app","text":"NODEPORT=$(kubectl get svc $service -o jsonpath='{.spec.ports[0].nodePort}') Now, visit the app at the url you get from the following command: echo \"http:// ${ CLUSTERIP } : ${ NODEPORT } \" in our browser A NodePort opens up all externally accessible nodes in the cluster at a fixed port. This is a little messy in practice, but good for demos, etc. because of its ease. After visiting our app, we can delete it and our service with: kubectl delete -f \" ${ MULTIARCH_HOME } \" /node-web-app/deployment.yaml deployment.extensions \"multi-arch-docker-node-web\" deleted kubectl delete -f \" ${ MULTIARCH_HOME } \" /node-web-app/service.yaml service \"multi-arch-docker-node-web\" deleted","title":"Visit Node Web App"},{"location":"5-Deploy-to-Kubernetes/#lets-create-our-first-go-deployment","text":"We'll use the example-go-server image and some quick kubectl one-line commands to spin things up. The advantage of these is that it is easy to get things running. However, your configuration isn't automatically saved and configured, which is why managing with .yaml files in production is preferred.","title":"Let's create our first Go Deployment"},{"location":"5-Deploy-to-Kubernetes/#make-a-quick-deployment-and-force-on-z","text":"kubectl run go-example --image = gmoney23/example-go-server --port 5000 --image-pull-policy = Always && kubectl patch deployment go-example --patch \" $( cat \" ${ MULTIARCH_HOME } \" /zNodeSelector.yaml ) \" kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead. deployment.apps/go-example created kubectl get deploy go-example Warning If you are using Docker on a non-z architecture, you will get a pending pod here along with the original running pod due to the inability to reschedule the pod to a node with the z (s390x) architecture. You can proceed as the original pod is still up and running so the workload faces no downtime.","title":"Make a Quick Deployment and force on z"},{"location":"5-Deploy-to-Kubernetes/#expose-the-deployment-to-the-outside-world","text":"This deployment is available. However, I can't access it from nodes outside of my cluster which is no good for a web app, so lets expose it to external connections by adding a nodePort service kubectl expose deployment go-example --type = NodePort kubectl get svc go-example Now if I go to my $CLUSTERIP:32532 as specified in port I can see my app running. To print out this as a url to go to like before run the following commands: NODEPORT=$(kubectl get svc go-example -o jsonpath='{.spec.ports[0].nodePort}') echo \"http://${CLUSTERIP}:${NODEPORT}\" Note Notice that this parsed the NodePort that was visible in the earlier output using jsonpath in order to easily print it for us with echo like before. Now I can save the deployment and service I created to a file with --export. kubectl get deployment go-example -o yaml --export > go-example-deployment.yaml kubectl get svc go-example -o yaml --export > go-example-svc.yaml","title":"Expose the Deployment to the Outside World"},{"location":"5-Deploy-to-Kubernetes/#find-the-node-hosting-my-pod","text":"Since this pod doesn't have a bash shell, since it was made from scratch, I'll use kubectl get pods -o wide to figure out which node it is running on. (We should know which architecture each host node is or could get that from our cluster admin.) kubectl get pods -l run = go-example -o wide","title":"Find the Node Hosting my Pod"},{"location":"5-Deploy-to-Kubernetes/#clean-up-sample-go-deployment","text":"We can delete the deployment and service by using kubectl delete resource name such as kubectl delete deployment deployment_name . For our sample go deployment this means: kubectl delete deployment go-example kubectl delete service go-example","title":"Clean Up Sample Go Deployment"},{"location":"5-Deploy-to-Kubernetes/#deal-with-proxies-with-a-configmap","text":"In our first 2 deployments, our apps were confined within our own premises, not requiring any calls to the outside world. However, in many cases, an app will need to make calls to the outside world for things like live updates and information from web sites. Soon, our outyet deployment will need to check the golang git page to see if version 1.11 of go has been released. Later, our href application will need to call out to websites to check their internal and external hrefs. Many applications require this functionality and many corporations have proxies that prevent apps running in containers from making these calls natively. In order to achieve this functionality, we will be using ConfigMaps. A configmap enables us to specify our configuration values in one place, where they can be referenced by an application. This way if a user needs different values for something such as a proxy, they can be changed in one place instead of digging into the application. Here is our proxy ConfigMap: Regular Users If a user doesn't need a proxy they can just not apply this ConfigMap since we will make it optional in the deployment. Proxy users Only if you use a proxy Please put your proxy values here and apply this ConfigMap with: kubectl apply -f \" ${ MULTIARCH_HOME } \" /proxy-configmap.yaml","title":"Deal with Proxies with a ConfigMap"},{"location":"5-Deploy-to-Kubernetes/#explore-how-to-create-deployments-and-services-from-yaml","text":"Here we will explore a sample deployment file to use in our cluster.","title":"Explore how to create deployments and services from yaml"},{"location":"5-Deploy-to-Kubernetes/#time-to-deploy-yaml","text":"This deployment attaches a label of small-outyet to identify it and selects pods with that same label running the gmoney23/smallest-outyet:1.0 image. The port (as we learned before with docker) is 8080 for the outyet family of applications. Additionally, the deployment references the configMap we previously created to deal with proxies. The envFrom means that all of the variables from the configMap will be deployed. The configMapRef marked as optional: true means that if you don't have a proxy and thus didn't apply the referenced configMap, you are not affected at all. This allows us to make an application that works for both proxy and regular users. Finally, the imagePullPolicy marked as Always means that we will check for a new version of the image every time we run a container from it. The default is that this happens for an image marked latest/no tag because that image is frequently updated with new versions. The default for images tagged with a version number is to only pull the image IfNotPresent which means if it is not on the machine. This is because usually a versioned release is updated with a new version rather than updating that same versioned image. If that's not the case, we can specify a different imagePullPolicy for those images like we do here with Always which overrides the default. If your image lies in a private repository, we can also add a secret to reference it without having to docker login to the repository first. See Pulling an image from a private registry . This can be important in Kubernetes, when many times the user should not have direct access to the host server and you want to be able to pull from a private registry with authentication. That was a mouthful ... Now, we can simply apply this file to create the deployment.","title":"Time to Deploy YAML"},{"location":"5-Deploy-to-Kubernetes/#time-to-service-yaml","text":"kubectl apply -f \" ${ MULTIARCH_HOME } \" /smallest-outyet/deployment.yaml deployment.extensions/multi-arch-docker-smallest-outyet created Now, to enable us to connect to this app, we need to deploy a service. We will create a yaml file for this as well called service.yaml . This service again uses the NodePort type mapping port 8080 as the container port and internal port to connect to the external NodePort . We can apply this to create the service. kubectl apply -f \" ${ MULTIARCH_HOME } \" /smallest-outyet/service.yaml service/multi-arch-docker-smallest-outyet created We can look at how the service maps to the pod by looking at the endpoints of the service. kubectl get ep -l app = smallest-outyet,lab = multi-arch-docker Now, I can scale the deployment to having 2 replicas instead of 1. deployment=multi-arch-docker-smallest-outyet kubectl patch deployment $deployment -p '{\"spec\": {\"replicas\": 2}}' Let's look at the endpoints for the service again. kubectl get ep -l app = smallest-outyet,lab = multi-arch-docker We can see the two pods' endpoints using the service instead of the one before.","title":"Time to Service YAML"},{"location":"5-Deploy-to-Kubernetes/#get-the-access-info-for-outyet","text":"To access the application itself, I can get the IP and NodePort using kubectl Admin Can run this command or check the console for the Proxy IP [ -z \" $CLUSTERIP \" ] && CLUSTERIP = $( kubectl get nodes -o jsonpath = \"{.items[0].status.addresses[0].address}\" ) Non-admin Needs to get the IP from Admin CLUSTERIP = <your_proxy_node_ip>","title":"Get the Access Info for Outyet"},{"location":"5-Deploy-to-Kubernetes/#time-to-see-if-its-outyet","text":"NODEPORT=$(kubectl get svc -l app=smallest-outyet,lab=multi-arch-docker -o jsonpath='{.items[0].spec.ports[0].nodePort}') echo \"http:// ${ CLUSTERIP } : ${ NODEPORT } \" I can plug this address into my browser to view the app.","title":"Time to See if it's Outyet"},{"location":"5-Deploy-to-Kubernetes/#clean-up","text":"To clean app I can delete the deployment using the yaml I created it with. The same goes for the service. kubectl delete -f \" ${ MULTIARCH_HOME } \" /smallest-outyet/deployment.yaml deployment.extensions \"multi-arch-docker-smallest-outyet\" deleted kubectl delete -f \" ${ MULTIARCH_HOME } \" /smallest-outyet/service.yaml service \"multi-arch-docker-smallest-outyet\" deleted","title":"Clean Up"},{"location":"5-Deploy-to-Kubernetes/#new-web-app-similar-steps-for-access","text":"","title":"New Web App, Similar Steps for Access:"},{"location":"5-Deploy-to-Kubernetes/#small-outyet-deserves-a-chance-to-run","text":"kubectl apply -f \" ${ MULTIARCH_HOME } \" /small-outyet/deployment.yaml deployment.extensions/multi-arch-docker-small-outyet created kubectl apply -f \" ${ MULTIARCH_HOME } \" /small-outyet/service.yaml service/multi-arch-docker-small-outyet created service = multi-arch-docker-small-outyet peer_pod = \" $( kubectl get pods -l app = small-outyet,lab = multi-arch-docker -o jsonpath = '{.items[*].metadata.name}' ) \"","title":"Small-Outyet Deserves a Chance to Run"},{"location":"5-Deploy-to-Kubernetes/#admin-can-run-this-command-or-to-find-out-the-proxy-ip","text":"[ -z \" $CLUSTERIP \" ] && CLUSTERIP = $( kubectl get nodes -o jsonpath = \"{.items[0].status.addresses[0].address}\" )","title":"Admin can run this command or to find out the Proxy IP"},{"location":"5-Deploy-to-Kubernetes/#non-admin-needs-to-get-the-ip-from-the-admin","text":"CLUSTERIP=<your_proxy_node_ip>","title":"Non-admin Needs to get the IP from the Admin"},{"location":"5-Deploy-to-Kubernetes/#its-still-out","text":"NODEPORT = $( kubectl get svc $service -o jsonpath = '{.spec.ports[0].nodePort}' ) echo \"http:// ${ CLUSTERIP } : ${ NODEPORT } \" We can visit this address, to visit our small-outyet app. We can follow similar steps for all of the outyets and example-go-server since they are all web applications with similar constraints.","title":"It's still out!"},{"location":"5-Deploy-to-Kubernetes/#clean-up-small-outyet","text":"kubectl delete -f \" ${ MULTIARCH_HOME } \" /small-outyet/deployment.yaml deployment.extensions \"multi-arch-docker-small-outyet\" deleted kubectl delete -f \" ${ MULTIARCH_HOME } \" /small-outyet/service.yaml service \"multi-arch-docker-small-outyet\" deleted","title":"Clean up Small-Outyet"},{"location":"5-Deploy-to-Kubernetes/#using-jobs-with-a-cronjob","text":"A job runs a number of pods in order to achieve a given number of successful completions at which point it is done. A CronJob runs a job on a time-based schedule. In this example, we will use a cronjob to launch our trusty href-counter , every minute. We will then change the environment values sent into href-counter, to get it to switch to a different website and look at its logs to show us the results. Instead of mounting the environment variables directly in the pod we will be using a new configmap and the one for proxies we created before to map configuration values to a container instead of hard coding them in (if applicable)","title":"Using Jobs with a CronJob"},{"location":"5-Deploy-to-Kubernetes/#first-lets-create-our-configmap","text":"Our configmap.yaml is as follows: It simply maps the http-url for us to hit with the href-tool to the site-url configmap with key http-url. We will be patching this when we want to update our container's environment value. Let's create our configmap: kubectl apply -f \" ${ MULTIARCH_HOME } \" /href-counter/configmap.yaml configmap/multi-arch-docker-site-url created Next, we have to make our CronJob. The following yaml will suffice: This CronJob schedules a job every minute using href-counter as our label and our image as gmoney23/href:1.0 . We also use our new ConfigMap in the valueFrom field of the env field where we specify our configMapKeyRef to reference a specific key. Finally, we connect to our proxy ConfigMap again since this app makes calls to outside websites. We should be all set. Time to make the cronjob. kubectl apply -f \" ${ MULTIARCH_HOME } \" /href-counter/cronjob.yaml cronjob.batch/multi-arch-docker-href-counter created After waiting a few minutes , run the logs to see the results: kubectl logs -l app = href-counter,lab = multi-arch-docker {\"internal\":5,\"external\":89} {\"internal\":5,\"external\":89} {\"internal\":5,\"external\":86} Finally, it's time to patch the configmap and see the changes... kubectl patch configmap multi-arch-docker-site-url -p '{\"data\": {\"http-url\": \"http://google.com\"}}' With the configmap patched we should be able to see the changes to the logs of the pods run by the jobs from the CronJob though you might have to wait up to a minute to see a change ( Note: usually only the 3 most recent pods at a given time are available for log reading ): kubectl logs -l app = href-counter,lab = multi-arch-docker {\"internal\":5,\"external\":89} {\"internal\":5,\"external\":86} {\"internal\":8,\"external\":11} Indeed, our values have changed. Our work is complete, time to clean up. kubectl delete -f \" ${ MULTIARCH_HOME } \" /href-counter/cronjob.yaml cronjob.batch \"multi-arch-docker-href-counter\" deleted kubectl delete -f \" ${ MULTIARCH_HOME } \" /href-counter/configmap.yaml configmap \"multi-arch-docker-site-url\" deleted In reality, our CronJob could be used to do anything from running the logs at the end of a day, to sending emails to a list of participants weekly, to running health checks on an application every minute to other automated tasks that need to be done every period of time. If you need more Kubernetes skills, cover your bases with Kubernetes basics . Additionally, if you have a hard time figuring out which api to use for a given type look no further","title":"First, lets create our ConfigMap"},{"location":"5-Deploy-to-Kubernetes/#closing-note","text":"THAT'S ALL FOLKS! Note to Self Oops that was off message. I meant to say: Knowing you have a guide as a template for future success fills you with determination . P.S. My ability to stay on message and reminisce about Undertale fills me with determination .* P.P.S What ever will I do next *","title":"Closing Note"},{"location":"5-Deploy-to-Kubernetes/#additional-topics","text":"After finishing everything here, check out: Packaging Applications and Services with Kubernetes Operators Red Hat OpenShift Free Interactive Online Learning","title":"Additional Topics"},{"location":"5-Deploy-to-Kubernetes/#home","text":"","title":"HOME"}]}